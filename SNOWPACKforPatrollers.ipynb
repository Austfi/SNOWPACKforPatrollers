{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Austfi/SNOWPACKforPatrollers/blob/dev/SNOWPACKforPatrollers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f3164ae"
      },
      "source": [
        "# SNOWPACK Model Setup and Execution\n",
        "\n",
        "This notebook guides you through the process of setting up and running the SNOWPACK model. It includes steps for installing necessary libraries, compiling the SNOWPACK and MeteoIO code, configuring the model, fetching meteorological data from historical weather models, and running SNOWPACK."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVNWofRBqYRU"
      },
      "source": [
        "# The following cells below set up SNOWPACK AND MeteoIO and the PATH structure to run it. They should not be edited. Pressing the play button below will run them all."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmAf9pWqC-Bc",
        "outputId": "2fd5098b-5d8f-47df-fe7e-b1fdd7d3a956"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cli.github.com/packages stable InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (185.1\r                                                                               \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (185.1\r                                                                               \rHit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.83)] [Waiting for headers] [Wai\r                                                                               \rHit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connecting to ppa.launchpadcont\r                                                                               \rGet:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\r0% [Waiting for headers] [5 InRelease 14.2 kB/129 kB 11%] [Waiting for headers]\r                                                                               \rGet:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,594 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,855 kB]\n",
            "Fetched 5,833 kB in 2s (3,699 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "liblapack-dev is already the newest version (3.10.0-2ubuntu1).\n",
            "numdiff is already the newest version (5.9.0-1).\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.15).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 47 not upgraded.\n",
            "Requirement already satisfied: openmeteo-requests in /usr/local/lib/python3.12/dist-packages (1.7.4)\n",
            "Requirement already satisfied: requests-cache in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: retry-requests in /usr/local/lib/python3.12/dist-packages (2.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: niquests>=3.15.2 in /usr/local/lib/python3.12/dist-packages (from openmeteo-requests) (3.15.2)\n",
            "Requirement already satisfied: openmeteo-sdk>=1.22.0 in /usr/local/lib/python3.12/dist-packages (from openmeteo-requests) (1.22.0)\n",
            "Requirement already satisfied: attrs>=21.2 in /usr/local/lib/python3.12/dist-packages (from requests-cache) (25.4.0)\n",
            "Requirement already satisfied: cattrs>=22.2 in /usr/local/lib/python3.12/dist-packages (from requests-cache) (25.3.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests-cache) (4.5.0)\n",
            "Requirement already satisfied: requests>=2.22 in /usr/local/lib/python3.12/dist-packages (from requests-cache) (2.32.4)\n",
            "Requirement already satisfied: url-normalize>=1.4 in /usr/local/lib/python3.12/dist-packages (from requests-cache) (2.2.1)\n",
            "Requirement already satisfied: urllib3>=1.25.5 in /usr/local/lib/python3.12/dist-packages (from requests-cache) (2.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: typing-extensions>=4.14.0 in /usr/local/lib/python3.12/dist-packages (from cattrs>=22.2->requests-cache) (4.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from niquests>=3.15.2->openmeteo-requests) (3.4.4)\n",
            "Requirement already satisfied: urllib3-future<3,>=2.13.903 in /usr/local/lib/python3.12/dist-packages (from niquests>=3.15.2->openmeteo-requests) (2.14.905)\n",
            "Requirement already satisfied: wassima<3,>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from niquests>=3.15.2->openmeteo-requests) (2.0.2)\n",
            "Requirement already satisfied: h11<1.0.0,>=0.11.0 in /usr/local/lib/python3.12/dist-packages (from urllib3-future<3,>=2.13.903->niquests>=3.15.2->openmeteo-requests) (0.16.0)\n",
            "Requirement already satisfied: jh2<6.0.0,>=5.0.3 in /usr/local/lib/python3.12/dist-packages (from urllib3-future<3,>=2.13.903->niquests>=3.15.2->openmeteo-requests) (5.0.10)\n",
            "Requirement already satisfied: qh3<2.0.0,>=1.5.4 in /usr/local/lib/python3.12/dist-packages (from urllib3-future<3,>=2.13.903->niquests>=3.15.2->openmeteo-requests) (1.5.5)\n",
            "Requirement already satisfied: flatbuffers==25.9.23 in /usr/local/lib/python3.12/dist-packages (from openmeteo-sdk>=1.22.0->openmeteo-requests) (25.9.23)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22->requests-cache) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22->requests-cache) (2025.10.5)\n"
          ]
        }
      ],
      "source": [
        "# @title Install environment updates\n",
        "!apt-get update\n",
        "!apt-get install -y build-essential cmake git liblapack-dev numdiff\n",
        "\n",
        "%pip install openmeteo-requests requests-cache retry-requests pandas numpy\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "from pathlib import Path\n",
        "import openmeteo_requests\n",
        "import requests_cache\n",
        "from retry_requests import retry\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import json\n",
        "from typing import Optional\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zMdG2rNTzJa",
        "outputId": "bd7e6ec4-597f-4d8f-8788-4de0c04a65f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'snowpack' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "# @title Download SNOWPACK\n",
        "!git clone https://github.com/snowpack-model/snowpack.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wo1YmVF7UEde",
        "outputId": "a7d54b4b-ab33-4761-d93a-1b8237159fa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory â€˜/root/usr/â€™: File exists\n"
          ]
        }
      ],
      "source": [
        "# @title Make  ~/usr folder and remove sample data\n",
        "!mkdir ~/usr/\n",
        "!rm -rf /content/sample_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3fe9cd6b",
        "outputId": "fd4119ef-0132-48e3-85d7-618b346ccf66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Could NOT find Doxygen (missing: DOXYGEN_EXECUTABLE) \n",
            "-- Could NOT find Doxygen (missing: DOXYGEN_EXECUTABLE) \n",
            "-- Configuring done (0.2s)\n",
            "-- Generating done (0.1s)\n",
            "-- Build files have been written to: /content/snowpack/Source/meteoio/build-meteoio\n",
            "[  0%] \u001b[32mBuilding C object meteoio/CMakeFiles/meteoio.dir/thirdParty/tinyexpr.c.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding C object meteoio/CMakeFiles/meteoio.dir/thirdParty/ctsa/autoutils.c.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding C object meteoio/CMakeFiles/meteoio.dir/thirdParty/ctsa/boxcox.c.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding C object meteoio/CMakeFiles/meteoio.dir/thirdParty/ctsa/boxjenkins.c.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding C object meteoio/CMakeFiles/meteoio.dir/thirdParty/ctsa/brent.c.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding C object meteoio/CMakeFiles/meteoio.dir/thirdParty/ctsa/conjgrad.c.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding C object meteoio/CMakeFiles/meteoio.dir/thirdParty/ctsa/conv.c.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding C object meteoio/CMakeFiles/meteoio.dir/thirdParty/ctsa/ctsa.c.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding C object meteoio/CMakeFiles/meteoio.dir/thirdParty/ctsa/dist.c.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding C object meteoio/CMakeFiles/meteoio.dir/thirdParty/ctsa/emle.c.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding C object meteoio/CMakeFiles/meteoio.dir/thirdParty/ctsa/erfunc.c.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding C object meteoio/CMakeFiles/meteoio.dir/thirdParty/ctsa/filter.c.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding C object meteoio/CMakeFiles/meteoio.dir/thirdParty/ctsa/hsfft.c.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding C object meteoio/CMakeFiles/meteoio.dir/thirdParty/ctsa/initest.c.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding C object meteoio/CMakeFiles/meteoio.dir/thirdParty/ctsa/lls.c.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding C object meteoio/CMakeFiles/meteoio.dir/thirdParty/ctsa/lnsrchmp.c.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding C object meteoio/CMakeFiles/meteoio.dir/thirdParty/ctsa/matrix.c.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding C object meteoio/CMakeFiles/meteoio.dir/thirdParty/ctsa/neldermead.c.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding C object meteoio/CMakeFiles/meteoio.dir/thirdParty/ctsa/newtonmin.c.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding C object meteoio/CMakeFiles/meteoio.dir/thirdParty/ctsa/nls.c.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding C object meteoio/CMakeFiles/meteoio.dir/thirdParty/ctsa/optimc.c.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding C object meteoio/CMakeFiles/meteoio.dir/thirdParty/ctsa/pdist.c.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding C object meteoio/CMakeFiles/meteoio.dir/thirdParty/ctsa/polyroot.c.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding C object meteoio/CMakeFiles/meteoio.dir/thirdParty/ctsa/pred.c.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding C object meteoio/CMakeFiles/meteoio.dir/thirdParty/ctsa/real.c.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding C object meteoio/CMakeFiles/meteoio.dir/thirdParty/ctsa/regression.c.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding C object meteoio/CMakeFiles/meteoio.dir/thirdParty/ctsa/secant.c.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding C object meteoio/CMakeFiles/meteoio.dir/thirdParty/ctsa/spectrum.c.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding C object meteoio/CMakeFiles/meteoio.dir/thirdParty/ctsa/stats.c.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding C object meteoio/CMakeFiles/meteoio.dir/thirdParty/ctsa/stl.c.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding C object meteoio/CMakeFiles/meteoio.dir/thirdParty/ctsa/talg.c.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding C object meteoio/CMakeFiles/meteoio.dir/thirdParty/ctsa/errors.c.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding C object meteoio/CMakeFiles/meteoio.dir/thirdParty/ctsa/unitroot.c.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding C object meteoio/CMakeFiles/meteoio.dir/thirdParty/ctsa/seastest.c.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding C object meteoio/CMakeFiles/meteoio.dir/thirdParty/ctsa/wtmath.c.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding C object meteoio/CMakeFiles/meteoio.dir/thirdParty/ctsa/wavefilt.c.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding C object meteoio/CMakeFiles/meteoio.dir/thirdParty/ctsa/wavelib.c.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding C object meteoio/CMakeFiles/meteoio.dir/thirdParty/ctsa/ctsa_wrapper.c.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object meteoio/CMakeFiles/meteoio.dir/dataClasses/Matrix.cc.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object meteoio/CMakeFiles/meteoio.dir/dataClasses/Grid2DObject.cc.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object meteoio/CMakeFiles/meteoio.dir/dataClasses/Grid3DObject.cc.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object meteoio/CMakeFiles/meteoio.dir/dataClasses/Date.cc.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object meteoio/CMakeFiles/meteoio.dir/dataClasses/CoordsAlgorithms.cc.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object meteoio/CMakeFiles/meteoio.dir/dataClasses/Coords.cc.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object meteoio/CMakeFiles/meteoio.dir/dataClasses/DEMObject.cc.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object meteoio/CMakeFiles/meteoio.dir/dataClasses/DEMAlgorithms.cc.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object meteoio/CMakeFiles/meteoio.dir/dataClasses/StationData.cc.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object meteoio/CMakeFiles/meteoio.dir/dataClasses/MeteoData.cc.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object meteoio/CMakeFiles/meteoio.dir/dataClasses/Buffer.cc.o\u001b[0m\n",
            "gmake[2]: *** [meteoio/CMakeFiles/meteoio.dir/build.make:737: meteoio/CMakeFiles/meteoio.dir/dataClasses/MeteoData.cc.o] Interrupt\n",
            "gmake[2]: *** [meteoio/CMakeFiles/meteoio.dir/build.make:751: meteoio/CMakeFiles/meteoio.dir/dataClasses/Buffer.cc.o] Interrupt\n",
            "gmake[1]: *** [CMakeFiles/Makefile2:379: meteoio/CMakeFiles/meteoio.dir/all] Interrupt\n",
            "gmake: *** [Makefile:156: all] Interrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "# @title Compile MeteoIO  (Runs about 6 Mins)\n",
        "!cd /content/snowpack/Source/meteoio/ && \\\n",
        "cmake -S . -B build-meteoio -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=$HOME/usr && \\\n",
        "cmake --build build-meteoio -j2 && \\\n",
        "cmake --install build-meteoio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "PM-xwrU9QNGD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2a138340",
        "outputId": "b33fb62e-20af-4a90-ed99-e8bd3d42cd13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Found METEOIO \n",
            "-- Found Lapack \n",
            "-- Could NOT find Doxygen (missing: DOXYGEN_EXECUTABLE) \n",
            "-- Could NOT find Doxygen (missing: DOXYGEN_EXECUTABLE) \n",
            "-- Configuring done (0.1s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /content/snowpack/Source/snowpack/build-snowpack\n",
            "[  6%] \u001b[32mBuilding CXX object snowpack/CMakeFiles/snowpack.dir/DataClasses.cc.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object snowpack/CMakeFiles/snowpack.dir/vanGenuchten.cc.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object snowpack/CMakeFiles/snowpack.dir/SnowpackConfig.cc.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object snowpack/CMakeFiles/snowpack.dir/Meteo.cc.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object snowpack/CMakeFiles/snowpack.dir/Saltation.cc.o\u001b[0m\n",
            "gmake[2]: *** [snowpack/CMakeFiles/snowpack.dir/build.make:121: snowpack/CMakeFiles/snowpack.dir/Meteo.cc.o] Interrupt\n",
            "gmake[2]: *** [snowpack/CMakeFiles/snowpack.dir/build.make:135: snowpack/CMakeFiles/snowpack.dir/Saltation.cc.o] Interrupt\n",
            "gmake[1]: *** [CMakeFiles/Makefile2:195: snowpack/CMakeFiles/snowpack.dir/all] Interrupt\n",
            "gmake: *** [Makefile:156: all] Interrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "# @title Compile SNOWPACK (Runs about 2 mins)\n",
        "\n",
        "!cd /content/snowpack/Source/snowpack/ && \\\n",
        "cmake -S . -B build-snowpack -DCMAKE_BUILD_TYPE=Release \\\n",
        "      -DCMAKE_PREFIX_PATH=$HOME/usr \\\n",
        "      -DDEBUG_ARITHM=OFF \\\n",
        "      -DENABLE_LAPACK=ON \\\n",
        "      -DMETEOIO_LIBRARY=$HOME/usr/lib/libmeteoio.so && \\\n",
        "cmake --build build-snowpack -j2 && \\\n",
        "cmake --install build-snowpack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "f54e3afa"
      },
      "outputs": [],
      "source": [
        "# @title Set up PATH to compiled code\n",
        "import os\n",
        "\n",
        "# Set PATH and LD_LIBRARY_PATH in Python environment\n",
        "# Note: These paths are for Colab. In local environments, snowpack is typically in /usr/local/bin\n",
        "os.environ[\"PATH\"] = \"/home/app/usr/bin:\" + os.environ.get(\"PATH\", \"\")\n",
        "os.environ[\"LD_LIBRARY_PATH\"] = \"/home/app/usr/lib:\" + os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n",
        "\n",
        "# Verify snowpack is accessible\n",
        "!which snowpack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yvdowMMw8XL"
      },
      "source": [
        "# The above cells have set up SNOWPACK and made it ready to run. The below cells create the files, collect data, and run the snowpack model. This generates snowprofiles with no snow to start the model from."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6Fxry_54s13l",
        "outputId": "da3985d8-dbf1-4807-d213-fd1d29b332ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SNOWPACK Configuration Files Generation\n",
            "==================================================\n",
            "Station: watrous_E_NTL (watrous)\n",
            "Location: 39.714380Â°N, -105.844750Â°W, 3596.6400000000003m\n",
            "Sno Directory: /content/input\n",
            "Ini Directory: /content/config\n",
            "==================================================\n",
            "\n",
            " Configuration files created:\n",
            "   .ini file: /content/config/watrous.ini\n",
            "   .sno files:\n",
            "     /content/input/keystone.sno\n",
            "     /content/input/keystone1.sno\n",
            "     /content/input/keystone2.sno\n",
            "     /content/input/keystone3.sno\n",
            "     /content/input/keystone4.sno\n",
            "\n",
            "Virtual slopes configured: 5\n",
            "   Slope 1: Flat (0Â°) -> keystone.sno\n",
            "   Slope 2: 38.0Â° slope facing North -> keystone1.sno\n",
            "   Slope 3: 38.0Â° slope facing East -> keystone2.sno\n",
            "   Slope 4: 38.0Â° slope facing South -> keystone3.sno\n",
            "   Slope 5: 38.0Â° slope facing West -> keystone4.sno\n",
            "\n",
            "SNOWPACK settings:\n",
            "   MEAS_TSS = false\n",
            "   ENFORCE_MEASURED_SNOW_HEIGHTS = false\n",
            "   NUMBER_SLOPES = 5\n",
            "\n",
            "ðŸŽ‰ SNOWPACK configuration generation complete!\n"
          ]
        }
      ],
      "source": [
        "#@title SNOWPACK Configuration Files Generation\n",
        "\"\"\"\n",
        "Generate .sno and .ini configuration files for SNOWPACK simulations\n",
        "\"\"\"\n",
        "\n",
        "#@markdown ## Station Configuration\n",
        "station_id = \"watrous\"  #@param {\"type\":\"string\"}\n",
        "station_name = \"watrous_E_NTL\"  #@param {\"type\":\"string\"}\n",
        "latitude = 39.71438  #@param {\"type\":\"number\"}\n",
        "longitude = -105.84475  #@param {\"type\":\"number\"}\n",
        "altitude = 11800  #@param {\"type\":\"integer\"}\n",
        "altitude_unit = \"feet\"  # @param [\"meters\", \"feet\"]\n",
        "timezone = -7  #@param {\"type\":\"number\"}\n",
        "profile_date = \"2024-11-01T00:00:00\"  #@param {\"type\":\"string\"}\n",
        "coord_sys = \"UTM\" #@param {\"type\":\"string\"}\n",
        "coord_param = \"13S\" #@param {\"type\":\"string\"}\n",
        "\n",
        "#@markdown ## Virtual Slopes Configuration\n",
        "num_slopes = 5  #@param {\"type\":\"number\", \"min\":1, \"max\":10}\n",
        "include_flat = True  #@param {\"type\":\"boolean\"}\n",
        "default_slope_angle = 38.0  #@param {\"type\":\"number\", \"min\":0, \"max\":90}\n",
        "\n",
        "#@markdown ## Slope Directions (degrees: 0=North, 90=East, 180=South, 270=West)\n",
        "north_slope = True  #@param {\"type\":\"boolean\"}\n",
        "east_slope = True  #@param {\"type\":\"boolean\"}\n",
        "south_slope = True  #@param {\"type\":\"boolean\"}\n",
        "west_slope = True  #@param {\"type\":\"boolean\"}\n",
        "custom_directions = \"\"  #@param {\"type\":\"string\"} {description:\"Comma-separated azimuth angles (e.g., 45,135,225,315)\"}\n",
        "\n",
        "#@markdown ## SNOWPACK Settings\n",
        "meas_tss = \"false\"  #@param [\"true\",\"false\"]\n",
        "enforce_measured_snow_heights = \"false\"  #@param [\"true\",\"false\"]\n",
        "write_profiles = \"true\"  #@param [\"true\",\"false\"]\n",
        "write_timeseries = \"false\"  #@param [\"true\",\"false\"]\n",
        "write_snowpack = \"false\"  #@param [\"true\",\"false\"]\n",
        "snow_erosion = \"redeposit\"  #@param [\"true\", \"redeposit\", \"free\", \"virtual\", \"none\"]\n",
        "snow_redistribution = \"advanced\"  #@param [\"true\", \"simple\", \"advanced\", \"none\"]\n",
        "hs_density_parameterization = \"LEHNING_NEW\"  #@param [\"LEHNING_OLD\", \"LEHNING_NEW\", \"BELLAIRE\", \"ZWART\", \"JORDY\", \"PAHAUT\", \"NIED\", \"VANKAMPENHOUT\", \"KRAMPE\"]\n",
        "\n",
        "#@markdown ## Output Configuration\n",
        "sno_directory = \"/content/input\"  #@param {\"type\":\"string\"}\n",
        "ini_directory = \"/content/config\"  #@param {\"type\":\"string\"}\n",
        "generate_config_files = True  #@param {\"type\":\"boolean\"}\n",
        "\n",
        "# @markdown ### SNOWPACK Run End Date\n",
        "snowpack_end_date_input = \"2025-04-01\"  # @param {type:\"date\"}\n",
        "snowpack_end_date = f\"{snowpack_end_date_input}T00:00\"\n",
        "\n",
        "# For local testing - use current directory\n",
        "import os\n",
        "if not os.path.exists(\"/content\"):\n",
        "    sno_directory = \".\"\n",
        "    ini_directory = \".\"\n",
        "\n",
        "def generate_slopes(include_flat, north_slope, east_slope, south_slope, west_slope, custom_directions, default_slope_angle):\n",
        "    \"\"\"Generate list of virtual slopes based on user selections\"\"\"\n",
        "    slopes = []\n",
        "\n",
        "    # Add flat slope if requested\n",
        "    if include_flat:\n",
        "        slopes.append((0.0, 0.0))\n",
        "\n",
        "    # Add cardinal direction slopes\n",
        "    if north_slope:\n",
        "        slopes.append((default_slope_angle, 0.0))\n",
        "    if east_slope:\n",
        "        slopes.append((default_slope_angle, 90.0))\n",
        "    if south_slope:\n",
        "        slopes.append((default_slope_angle, 180.0))\n",
        "    if west_slope:\n",
        "        slopes.append((default_slope_angle, 270.0))\n",
        "\n",
        "    # Add custom directions\n",
        "    if custom_directions.strip():\n",
        "        try:\n",
        "            custom_angles = [float(x.strip()) for x in custom_directions.split(',')]\n",
        "            for angle in custom_angles:\n",
        "                if 0 <= angle <= 360:\n",
        "                    slopes.append((default_slope_angle, angle))\n",
        "        except ValueError:\n",
        "            print(\"Warning: Invalid custom directions format. Use comma-separated numbers.\")\n",
        "\n",
        "    return slopes\n",
        "\n",
        "def to_meters(value: float, unit: str) -> float:\n",
        "    \"\"\"Normalize altitude inputs so downstream code always works in meters.\"\"\"\n",
        "    unit = unit.lower()\n",
        "    if unit == \"meters\":\n",
        "        return float(value)\n",
        "    if unit == \"feet\":\n",
        "        return float(value) * 0.3048\n",
        "    raise ValueError(f\"Unsupported altitude unit: {unit}\")\n",
        "\n",
        "# Using variables from the configuration cell\n",
        "altitude_meters = to_meters(altitude, altitude_unit)\n",
        "\n",
        "def create_sno_content(station_id, station_name, longitude, latitude, altitude_meters, timezone, profile_date, slope_angle, slope_azimuth):\n",
        "    \"\"\"Create .sno file content for a single slope\"\"\"\n",
        "\n",
        "    content = f\"\"\"SMET 1.1 ASCII\n",
        "[HEADER]\n",
        "station_id       = {station_id}\n",
        "station_name     = {station_name}\n",
        "longitude        = {longitude}\n",
        "latitude         = {latitude}\n",
        "altitude         = {altitude_meters}\n",
        "nodata           = -999\n",
        "tz               = {timezone}\n",
        "source           = OpenMeteo\n",
        "prototype        = SNOWPACK\n",
        "ProfileDate      = {profile_date}\n",
        "HS_Last          = 0.0000\n",
        "SlopeAngle       = {slope_angle}\n",
        "SlopeAzi         = {slope_azimuth}\n",
        "nSoilLayerData   = 0\n",
        "nSnowLayerData   = 0\n",
        "SoilAlbedo       = 0.09\n",
        "BareSoil_z0      = 0.020\n",
        "CanopyHeight     = 0.00\n",
        "CanopyLeafAreaIndex = 0.00\n",
        "CanopyDirectThroughfall = 1.00\n",
        "ErosionLevel     = 0\n",
        "TimeCountDeltaHS = 0.000000\n",
        "WindScalingFactor = 1.00\n",
        "\n",
        "fields           = timestamp Layer_Thick  T  Vol_Frac_I  Vol_Frac_W  Vol_Frac_V  Vol_Frac_S Rho_S Conduc_S HeatCapac_S  rg  rb  dd  sp  mk mass_hoar ne CDot metamo\n",
        "[DATA]\n",
        "\"\"\"\n",
        "\n",
        "    return content\n",
        "\n",
        "def create_ini_content(station_id, filename, snowfiles, meas_tss, enforce_measured_snow_heights,\n",
        "                        coord_sys, coord_param, timezone, write_profiles, write_timeseries, write_snowpack,\n",
        "                        snow_erosion, snow_redistribution, hs_density_parameterization,\n",
        "                        meteo_path=\"../input\", output_path=\"./output\", snow_output_path=\"./output\",\n",
        "                        ts_days_between=0.25, prof_days_between=1, snow_days_between=365, first_backup=365):\n",
        "    \"\"\"Create .ini file content that mirrors the master configuration structure.\"\"\"\n",
        "\n",
        "    def format_bool(value):\n",
        "        if isinstance(value, str):\n",
        "            return \"TRUE\" if value.strip().lower() == \"true\" else \"FALSE\"\n",
        "        return \"TRUE\" if bool(value) else \"FALSE\"\n",
        "\n",
        "    def format_option(value):\n",
        "        return str(value).strip().upper()\n",
        "\n",
        "    def format_number(value):\n",
        "        return format(value, \"g\") if isinstance(value, (int, float)) else str(value)\n",
        "\n",
        "    meteo_path = meteo_path or \"../input\"\n",
        "    output_path = output_path or \"./output\"\n",
        "    snow_output_path = snow_output_path or output_path\n",
        "\n",
        "    lines = []\n",
        "    lines.append(\"############################## INPUTS ##############################\")\n",
        "    lines.append(\"\")\n",
        "    lines.append(\"[INPUT]\")\n",
        "    lines.append(f\"COORDSYS={coord_sys}\")\n",
        "    lines.append(f\"COORDPARAM={coord_param}\")\n",
        "    lines.append(f\"TIME_ZONE={timezone}\")\n",
        "    lines.append(\"METEO=SMET\")\n",
        "    lines.append(\"SNOW=SMET\")\n",
        "    lines.append(\"SLOPE_FROM_SNO=TRUE\")\n",
        "    lines.append(f\"METEOPATH={meteo_path}\")\n",
        "    lines.append(f\"METEOFILE1={filename}\")\n",
        "    for index, snowfile in enumerate(snowfiles, start=1):\n",
        "        lines.append(f\"SNOWFILE{index}={snowfile}\")\n",
        "\n",
        "    lines.append(\"\")\n",
        "    lines.append(\"[FILTERS]\")\n",
        "    lines.append(\"TA::FILTER1=ADD\")\n",
        "    lines.append(\"TA::ARG1::TYPE=CST\")\n",
        "    lines.append(\"TA::ARG1::CST=273.15\")\n",
        "    lines.append(\"RH::FILTER1=MULT\")\n",
        "    lines.append(\"RH::ARG1::TYPE=CST\")\n",
        "    lines.append(\"RH::ARG1::CST=0.01\")\n",
        "\n",
        "    lines.append(\"\")\n",
        "    lines.append(\"[INPUTEDITING]\")\n",
        "    lines.append(\"*::EDIT1=CREATE\")\n",
        "    lines.append(\"*::ARG1::PARAM=TSG\")\n",
        "    lines.append(\"*::ARG1::ALGORITHM=CST\")\n",
        "    lines.append(\"*::ARG1::VALUE=273.15\")\n",
        "\n",
        "    lines.append(\"\")\n",
        "    lines.append(\"[INTERPOLATIONS1D]\")\n",
        "    lines.append(\"MAX_GAP_SIZE=86400\")\n",
        "    lines.append(\"PSUM::RESAMPLE1=ACCUMULATE\")\n",
        "    lines.append(\"PSUM::ARG1::PERIOD=1800\")\n",
        "    lines.append(\"DW::RESAMPLE1=NEAREST\")\n",
        "    lines.append(\"ILWR::RESAMPLE1=LINEAR\")\n",
        "    lines.append(\"ISWR::RESAMPLE1=LINEAR\")\n",
        "    lines.append(\"RH::RESAMPLE1=LINEAR\")\n",
        "    lines.append(\"TA::RESAMPLE1=LINEAR\")\n",
        "    lines.append(\"VW::RESAMPLE1=LINEAR\")\n",
        "\n",
        "    lines.append(\"\")\n",
        "    lines.append(\"############################## MODEL ##############################\")\n",
        "    lines.append(\"\")\n",
        "    lines.append(\"[SNOWPACK]\")\n",
        "    lines.append(\"CALCULATION_STEP_LENGTH=30\")\n",
        "    lines.append(\"ROUGHNESS_LENGTH=0.002\")\n",
        "    lines.append(\"HEIGHT_OF_METEO_VALUES=2\")\n",
        "    lines.append(\"HEIGHT_OF_WIND_VALUE=10\")\n",
        "    lines.append(f\"ENFORCE_MEASURED_SNOW_HEIGHTS={format_bool(enforce_measured_snow_heights)}\")\n",
        "    lines.append(\"SW_MODE=INCOMING\")\n",
        "    lines.append(\"ATMOSPHERIC_STABILITY=NEUTRAL\")\n",
        "    lines.append(\"CHANGE_BC=FALSE\")\n",
        "    lines.append(\"SNP_SOIL=FALSE\")\n",
        "    lines.append(\"CANOPY=FALSE\")\n",
        "    lines.append(f\"MEAS_TSS={format_bool(meas_tss)}\")\n",
        "\n",
        "    lines.append(\"\")\n",
        "    lines.append(\"[SNOWPACKADVANCED]\")\n",
        "    lines.append(f\"NUMBER_SLOPES={len(snowfiles)}\")\n",
        "    lines.append(f\"SNOW_REDISTRIBUTION={format_option(snow_redistribution)} # Specifies if redistribution of snow is allowed from windward (luv) to lee slopes. The snow erosion on virtual slopes will contribute to the drifting snow index (default: FALSE).\")\n",
        "    lines.append(\"THRESH_RAIN=1.4          # Increase default from 1.2 to 1.4 based on feedback from Sherbrooke and Zach Miller\")\n",
        "    lines.append(\"T_CRAZY_MIN=140\")\n",
        "    lines.append(\"T_CRAZY_MAX=360\")\n",
        "    lines.append(\"SSI_IS_RTA=TRUE         # Assme this is helpful for Flo's code?\")\n",
        "    lines.append(f\"SNOW_EROSION={format_option(snow_erosion)}\")\n",
        "    lines.append(f\"HS_DENSITY_PARAMETERIZATION={format_option(hs_density_parameterization)}\")\n",
        "\n",
        "    lines.append(\"\")\n",
        "    lines.append(\"############################## OUTPUTS ##############################\")\n",
        "    lines.append(\"\")\n",
        "    lines.append(\"[OUTPUT]\")\n",
        "    lines.append(\"## Smet settings\")\n",
        "    lines.append(\"METEO=SMET\")\n",
        "    lines.append(\"SMET_APPEND=TRUE\")\n",
        "    lines.append(\"SMET_OVERWRITE=FALSE\")\n",
        "    lines.append(\"SMET_PLOT_HEADERS=FALSE\")\n",
        "    lines.append(f\"COORDSYS={coord_sys if coord_sys else 'PROJ4'}\")\n",
        "    lines.append(f\"COORDPARAM={coord_param if coord_param else '4326'}\")\n",
        "    lines.append(f\"TIME_ZONE={timezone}\")\n",
        "\n",
        "    lines.append(\"## Timeseries smet files\")\n",
        "    lines.append(f\"TS_WRITE={format_bool(write_timeseries)}           # Writes to METEOPATH\")\n",
        "    lines.append(\"TS_FORMAT=SMET\")\n",
        "    lines.append(\"TS_START=0\")\n",
        "    lines.append(f\"TS_DAYS_BETWEEN={format_number(ts_days_between)}    # 3 hours=.125, 1 hour=4.1666e-2, 0.5 hour=2.08333e-2, 0.25 hour=1.04167e-2\")\n",
        "    lines.append(\"AVGSUM_TIME_SERIES=FALSE # If AVGSUM_TIME_SERIES is set, mean fluxes and cumulated masses since last dump are written, else current energy fluxes, cumulated masses over last COMPUTATION_STEP_LENGTH (recommended setting in operational mode). This is forced to false when using virtual slopes!\")\n",
        "    lines.append(\"CUMSUM_MASS=FALSE       # If CUMSUM_MASS is set, current value of cumulated masses since begin of run are dumped. See PRECIP_RATES for precipitation. This is forced to false when using virtual slopes!\")\n",
        "    lines.append(\"PRECIP_RATES=FALSE      # Write precipitation as rates (kg m-2 h-1, default) or as sums over the output time step. Please note that the units label in the output file and in SN_GUI will remain kg m-2 h-1 and will therefore not match sums.\")\n",
        "    lines.append(\"OUT_HAZ=TRUE\")\n",
        "    lines.append(\"OUT_HEAT=FALSE\")\n",
        "    lines.append(\"OUT_LW=FALSE\")\n",
        "    lines.append(\"OUT_METEO=TRUE\")\n",
        "    lines.append(\"OUT_SW=FALSE\")\n",
        "    lines.append(\"OUT_STAB=FALSE\")\n",
        "    lines.append(\"OUT_T=FALSE\")\n",
        "\n",
        "    lines.append(\"## Pro files\")\n",
        "    lines.append(f\"PROF_WRITE={format_bool(write_profiles)}         # Writes to METEOPATH\")\n",
        "    lines.append(\"PROF_FORMAT=PRO\")\n",
        "    lines.append(\"PROF_AGE_OR_DATE=DATE\")\n",
        "    lines.append(f\"PROF_DAYS_BETWEEN={format_number(prof_days_between)}\")\n",
        "\n",
        "    lines.append(\"## Sno files\")\n",
        "    lines.append(f\"SNOW_WRITE={format_bool(write_snowpack)}         # Writes to SNOWPATH\")\n",
        "    lines.append(\"SNOW=SMET\")\n",
        "    lines.append(f\"SNOWPATH={snow_output_path}\")\n",
        "    lines.append(f\"SNOW_DAYS_BETWEEN={format_number(snow_days_between)}   # Dump snow files every SNOW_DAYS_BETWEEN day (STATION_YYYYMMDDHHMMSS.sno)\")\n",
        "    lines.append(f\"FIRST_BACKUP={format_number(first_backup)}        # First dump of sno files happens at end of run or after FIRST_BACKUP days, whichever happens first\")\n",
        "\n",
        "    lines.append(\"## Haz files\")\n",
        "    lines.append(\"HAZ_WRITE=FALSE         # Writes to SNOWPATH with same dump settings as SNOW\")\n",
        "    lines.append(f\"METEOPATH={output_path}\")\n",
        "\n",
        "    return \"\\n\".join(lines) + \"\\n\"\n",
        "\n",
        "def get_slope_filename(angle, azimuth, slope_index, include_flat):\n",
        "    \"\"\"Get filename for slope naming convention.\n",
        "\n",
        "    Naming rules:\n",
        "    - Flat slope (angle == 0.0): Returns 'keystone' (no number)\n",
        "    - Non-flat slopes: Returns 'keystone1', 'keystone2', etc.\n",
        "    - Numbering depends on whether flat slope is included:\n",
        "      * If flat included: non-flat slopes start from index 1\n",
        "      * If flat not included: all slopes start from index 1\n",
        "\n",
        "    Args:\n",
        "        angle: Slope angle in degrees (0.0 for flat)\n",
        "        azimuth: Slope azimuth in degrees\n",
        "        slope_index: Index of slope in the slopes list\n",
        "        include_flat: Whether flat slope is included in the list\n",
        "\n",
        "    Returns:\n",
        "        Filename base (without .sno extension)\n",
        "    \"\"\"\n",
        "    if angle == 0.0:\n",
        "        return \"keystone\"  # Flat slope gets no number\n",
        "    else:\n",
        "        # For numbered slopes, determine the number based on position\n",
        "        if include_flat:\n",
        "            # If flat is included, non-flat slopes start from index 1\n",
        "            return f\"keystone{slope_index}\"\n",
        "        else:\n",
        "            # If no flat, start numbering from 1\n",
        "            return f\"keystone{slope_index + 1}\"\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "\n",
        "    print(\"SNOWPACK Configuration Files Generation\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Station: {station_name} ({station_id})\")\n",
        "    print(f\"Location: {latitude:.6f}Â°N, {longitude:.6f}Â°W, {altitude_meters}m\")\n",
        "    print(f\"Sno Directory: {sno_directory}\")\n",
        "    print(f\"Ini Directory: {ini_directory}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    if not generate_config_files:\n",
        "        print(\"Configuration generation disabled. Set 'generate_config_files' to True to generate files.\")\n",
        "        return\n",
        "\n",
        "    # Validate profile_date format\n",
        "    try:\n",
        "        from datetime import datetime\n",
        "        datetime.strptime(profile_date, \"%Y-%m-%dT%H:%M:%S\")\n",
        "    except ValueError:\n",
        "        print(f\"Warning: profile_date format may be incorrect. Expected format: YYYY-MM-DDTHH:MM:SS\")\n",
        "        print(f\"  Current value: {profile_date}\")\n",
        "\n",
        "    # Validate directories\n",
        "    try:\n",
        "        os.makedirs(sno_directory, exist_ok=True)\n",
        "        os.makedirs(ini_directory, exist_ok=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating directories: {e}\")\n",
        "        return\n",
        "\n",
        "    output_directory_path = os.path.join(ini_directory, \"output\")\n",
        "    os.makedirs(output_directory_path, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        meteo_path_relative = os.path.relpath(sno_directory, ini_directory)\n",
        "    except ValueError:\n",
        "        meteo_path_relative = sno_directory\n",
        "\n",
        "    output_path_relative = \"./output\"\n",
        "    snow_output_relative = \"./output\"\n",
        "\n",
        "    # Generate slopes\n",
        "    slopes = generate_slopes(include_flat, north_slope, east_slope, south_slope, west_slope, custom_directions, default_slope_angle)\n",
        "\n",
        "    # Limit to requested number of slopes\n",
        "    if len(slopes) > num_slopes:\n",
        "        slopes = slopes[:num_slopes]\n",
        "        print(f\"Warning: Limited to {num_slopes} slopes as requested\")\n",
        "\n",
        "    # Create multiple .sno files and collect snowfile names\n",
        "    snowfiles = []\n",
        "    sno_files_created = []\n",
        "\n",
        "    for i, (angle, azimuth) in enumerate(slopes):\n",
        "        # Get filename based on new naming convention\n",
        "        sno_filename = get_slope_filename(angle, azimuth, i, include_flat) + \".sno\"\n",
        "        snowfiles.append(sno_filename)\n",
        "\n",
        "        # Create .sno content for this slope\n",
        "        sno_content = create_sno_content(station_id, station_name, longitude, latitude, altitude_meters, timezone, profile_date, angle, azimuth)\n",
        "\n",
        "        # Write .sno file to input directory\n",
        "        sno_filepath = os.path.join(sno_directory, sno_filename)\n",
        "        try:\n",
        "            with open(sno_filepath, \"w\") as f:\n",
        "                f.write(sno_content)\n",
        "            sno_files_created.append(sno_filepath)\n",
        "        except Exception as e:\n",
        "            print(f\"Error writing {sno_filepath}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Create .ini content with multiple SNOWFILE entries\n",
        "    ini_content = create_ini_content(\n",
        "        station_id,\n",
        "        f\"{station_id}.smet\",\n",
        "        snowfiles,\n",
        "        meas_tss,\n",
        "        enforce_measured_snow_heights,\n",
        "        coord_sys,\n",
        "        coord_param,\n",
        "        timezone,\n",
        "        write_profiles,\n",
        "        write_timeseries,\n",
        "        write_snowpack,\n",
        "        snow_erosion,\n",
        "        snow_redistribution,\n",
        "        hs_density_parameterization,\n",
        "        meteo_path=meteo_path_relative,\n",
        "        output_path=output_path_relative,\n",
        "        snow_output_path=snow_output_relative\n",
        "    )\n",
        "\n",
        "    # Write .ini file to keystone directory\n",
        "    ini_filename = os.path.join(ini_directory, f\"{station_id}.ini\")\n",
        "    try:\n",
        "        with open(ini_filename, \"w\") as f:\n",
        "            f.write(ini_content)\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing {ini_filename}: {e}\")\n",
        "        return\n",
        "\n",
        "    # Display results\n",
        "    print(f\"\\n Configuration files created:\")\n",
        "    print(f\"   .ini file: {ini_filename}\")\n",
        "    print(f\"   .sno files:\")\n",
        "    for sno_file in sno_files_created:\n",
        "        print(f\"     {sno_file}\")\n",
        "\n",
        "    print(f\"\\nVirtual slopes configured: {len(slopes)}\")\n",
        "    for i, (angle, azimuth) in enumerate(slopes):\n",
        "        sno_filename = get_slope_filename(angle, azimuth, i, include_flat)\n",
        "        if angle == 0.0:\n",
        "            print(f\"   Slope {i+1}: Flat (0Â°) -> {sno_filename}.sno\")\n",
        "        else:\n",
        "            direction = \"\"\n",
        "            if azimuth == 0.0:\n",
        "                direction = \"North\"\n",
        "            elif azimuth == 90.0:\n",
        "                direction = \"East\"\n",
        "            elif azimuth == 180.0:\n",
        "                direction = \"South\"\n",
        "            elif azimuth == 270.0:\n",
        "                direction = \"West\"\n",
        "            else:\n",
        "                direction = f\"{azimuth}Â°\"\n",
        "            print(f\"   Slope {i+1}: {angle}Â° slope facing {direction} -> {sno_filename}.sno\")\n",
        "\n",
        "    print(f\"\\nSNOWPACK settings:\")\n",
        "    print(f\"   MEAS_TSS = {meas_tss}\")\n",
        "    print(f\"   ENFORCE_MEASURED_SNOW_HEIGHTS = {enforce_measured_snow_heights}\")\n",
        "    print(f\"   NUMBER_SLOPES = {len(slopes)}\")\n",
        "    print(f\"   SNOW_EROSION = {snow_erosion}\")\n",
        "    print(f\"   SNOW_REDISTRIBUTION = {snow_redistribution}\")\n",
        "    print(f\"   HS_DENSITY_PARAMETERIZATION = {hs_density_parameterization}\")\n",
        "\n",
        "    print(\"\\nðŸŽ‰ SNOWPACK configuration generation complete!\")\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Master INI defaults\n",
        "\n",
        "The configuration files generated below now copy the structure and defaults from `master.ini`. You can still change site-specific details (location, time zone, slope files, relative paths) through the form inputs in the cell above. If you need to tweak any of the master defaults, adjust the parameters passed into `create_ini_content` where noted in the code comments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1tDfNJYn15FI",
        "outputId": "d26dafba-fe68-4b0b-bce1-d5c5ddcfa814"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SMET Generation Parameters\n",
            "==================================================\n",
            "Location: 39.71438, -105.84475\n",
            "Altitude input: 11800 feet (3596.64 m used for calculations)\n",
            "Station: watrous_E_NTL\n",
            "Period: 2024-11-01 to 2025-04-30\n",
            "Model: ifs\n",
            "Generate: True\n",
            "Open-Meteo elevation: 3596.64 m (selected altitude)\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# @title SMET Handling, API Setup, and Parameters\n",
        "\n",
        "# @markdown ### Location Settings are used from above.\n",
        "# Pulling from the configuration cell\n",
        "# latitude = 39.56858687967004  # @param {type:\"number\"}\n",
        "# longitude = -105.91900397453021  # @param {type:\"number\"}\n",
        "# altitude_input = 3614  # @param {type:\"number\"}\n",
        "# altitude_unit = \"meters\"  # @param [\"meters\", \"feet\"]\n",
        "\n",
        "# @markdown ### Station Information is used from above.\n",
        "# Pulling from the configuration cell\n",
        "# station_name = \"keystone_model\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown ### Time Period\n",
        "start_date = \"2024-11-01\"  # @param {type:\"date\"}\n",
        "end_date = \"2025-04-30\"    # @param {type:\"date\"}\n",
        "\n",
        "# @markdown ### Weather Model\n",
        "model_selection = \"ifs\"  # @param [\"nbm\", \"ifs\", \"gfs\", \"gfs_hrrr\"]\n",
        "\n",
        "# @markdown ### HS (Snow Depth) Source\n",
        "hs_source = \"model\"  # @param [\"model\", \"snodas\"]\n",
        "\n",
        "# @markdown ### Open-Meteo Elevation\n",
        "openmeteo_elevation_mode = \"use_selected_altitude\"  # @param [\"use_model_elevation\", \"use_selected_altitude\"]\n",
        "\n",
        "# @markdown ### SMET Generation\n",
        "generate_files = True  # @param {type:\"boolean\"}\n",
        "\n",
        "# --- Helper functions ----------------------------------------------------\n",
        "\n",
        "# Import additional modules needed for SNODAS\n",
        "import struct\n",
        "import tarfile\n",
        "import gzip\n",
        "import urllib.request\n",
        "import urllib.error\n",
        "from io import BytesIO\n",
        "\n",
        "\n",
        "def get_snodas_snow_depth(lat, lon, date_str, cache_dir=\"snodas_cache\", debug=False):\n",
        "    \"\"\"\n",
        "    Download and extract SNODAS snow depth from NSIDC.\n",
        "    \n",
        "    SNODAS Units:\n",
        "    - Raw data: millimeters (mm) stored as integer (snow_depth_raw / 1000.0)\n",
        "    - Returns: meters (m) as float\n",
        "    \n",
        "    This is different from Open-Meteo which returns centimeters (cm).\n",
        "    \"\"\"\n",
        "    SNODAS_NODATA = -9999\n",
        "    \n",
        "    # Grid configurations (detected from file size)\n",
        "    GRID_CONFIGS = {\n",
        "        'old': {'XMIN': -124.73375000000000, 'YMAX': 52.87458333333333, \n",
        "                'XMAX': -66.94208333333333, 'YMIN': 24.94958333333333,\n",
        "                'NCOLS': 6935, 'NROWS': 3351, 'name': 'Pre-Oct-2013'},\n",
        "        'new': {'XMIN': -124.73333333333333, 'YMAX': 52.87500000000000,\n",
        "                'XMAX': -66.94166666666667, 'YMIN': 24.95000000000000,\n",
        "                'NCOLS': 3353, 'NROWS': 3353, 'name': 'Post-Oct-2013'}\n",
        "    }\n",
        "    \n",
        "    # Check location bounds\n",
        "    if lat < 24.95 or lat > 52.88 or lon < -124.74 or lon > -66.94:\n",
        "        return None\n",
        "    \n",
        "    # Construct URL\n",
        "    tar_filename = f\"SNODAS_{date_str}.tar\"\n",
        "    data_base = \"https://noaadata.apps.nsidc.org/NOAA/G02158/masked\"\n",
        "    year = date_str[:4]\n",
        "    month = date_str[4:6]\n",
        "    month_names = [\"01_Jan\", \"02_Feb\", \"03_Mar\", \"04_Apr\", \"05_May\", \"06_Jun\",\n",
        "                   \"07_Jul\", \"08_Aug\", \"09_Sep\", \"10_Oct\", \"11_Nov\", \"12_Dec\"]\n",
        "    month_dir = month_names[int(month) - 1]\n",
        "    data_url = f\"{data_base}/{year}/{month_dir}/{tar_filename}\"\n",
        "    \n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    cache_path = os.path.join(cache_dir, tar_filename)\n",
        "    \n",
        "    try:\n",
        "        # Download or use cache\n",
        "        if os.path.exists(cache_path):\n",
        "            with open(cache_path, 'rb') as f:\n",
        "                tar_data = BytesIO(f.read())\n",
        "        else:\n",
        "            if debug:\n",
        "                print(f\"  Downloading {date_str}...\")\n",
        "            with urllib.request.urlopen(data_url, timeout=60) as response:\n",
        "                tar_data = BytesIO(response.read())\n",
        "                with open(cache_path, 'wb') as f:\n",
        "                    f.write(tar_data.getvalue())\n",
        "            tar_data.seek(0)\n",
        "        \n",
        "        # Extract and decompress\n",
        "        with tarfile.open(fileobj=tar_data, mode='r') as tar:\n",
        "            for member in tar.getmembers():\n",
        "                if '1036' in member.name and member.name.endswith('.dat.gz'):\n",
        "                    snow_depth_gz_file = tar.extractfile(member)\n",
        "                    break\n",
        "            else:\n",
        "                return None\n",
        "        \n",
        "        with gzip.open(snow_depth_gz_file, 'rb') as gz_file:\n",
        "            data = gz_file.read()\n",
        "        \n",
        "        # Detect grid from file size\n",
        "        num_values = len(data) // 2\n",
        "        grid_config = None\n",
        "        for config in GRID_CONFIGS.values():\n",
        "            if num_values == config['NCOLS'] * config['NROWS']:\n",
        "                grid_config = config\n",
        "                break\n",
        "        \n",
        "        if grid_config is None:\n",
        "            return None\n",
        "        \n",
        "        # Parse binary data\n",
        "        SNODAS_NCOLS = grid_config['NCOLS']\n",
        "        SNODAS_NROWS = grid_config['NROWS']\n",
        "        values = struct.unpack(f\">{SNODAS_NCOLS * SNODAS_NROWS}h\", data)\n",
        "        snow_depth_array = np.array(values).reshape((SNODAS_NROWS, SNODAS_NCOLS))\n",
        "        \n",
        "        # Calculate grid coordinates\n",
        "        SNODAS_XMIN = grid_config['XMIN']\n",
        "        SNODAS_YMAX = grid_config['YMAX']\n",
        "        SNODAS_CELLSIZE_X = (grid_config['XMAX'] - SNODAS_XMIN) / SNODAS_NCOLS\n",
        "        SNODAS_CELLSIZE_Y = (SNODAS_YMAX - grid_config['YMIN']) / SNODAS_NROWS\n",
        "        \n",
        "        col = int((lon - SNODAS_XMIN) / SNODAS_CELLSIZE_X)\n",
        "        row = int((SNODAS_YMAX - lat) / SNODAS_CELLSIZE_Y)\n",
        "        col = max(0, min(SNODAS_NCOLS - 1, col))\n",
        "        row = max(0, min(SNODAS_NROWS - 1, row))\n",
        "        \n",
        "        # Extract value\n",
        "        snow_depth_raw = snow_depth_array[row, col]\n",
        "        if snow_depth_raw == SNODAS_NODATA or snow_depth_raw < 0:\n",
        "            return None\n",
        "        \n",
        "        snow_depth_m = snow_depth_raw / 1000.0\n",
        "        return snow_depth_m if snow_depth_m >= 0.01 else 0.0\n",
        "            \n",
        "    except Exception as e:\n",
        "        if debug:\n",
        "            print(f\"  Error: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def resolve_openmeteo_elevation(mode: str, station_alt_m: float) -> Optional[float]:\n",
        "    \"\"\"\n",
        "    Decide which elevation to send to the Open-Meteo API.\n",
        "    - use_model_elevation     -> None (API uses its DEM; statistical downscaling stays enabled)\n",
        "    - use_selected_altitude   -> the altitude value converted to meters above\n",
        "    \"\"\"\n",
        "    mode = mode.lower()\n",
        "    if mode == \"use_model_elevation\":\n",
        "        return None\n",
        "    if mode == \"use_selected_altitude\":\n",
        "        return float(station_alt_m)\n",
        "    raise ValueError(f\"Unknown elevation mode: {mode}\")\n",
        "\n",
        "\n",
        "def create_smet_from_weather_data(weather_df: pd.DataFrame,\n",
        "                                  output_path: str,\n",
        "                                  station_id: str,\n",
        "                                  station_name: str,\n",
        "                                  latitude: float,\n",
        "                                  longitude: float,\n",
        "                                  altitude_meters: float,\n",
        "                                  timezone: float = -7) -> None:\n",
        "    \"\"\"\n",
        "    Create a SMET file from a weather DataFrame and write it to disk.\n",
        "    \n",
        "    Expected DataFrame Units (SMET format):\n",
        "    - TA: Kelvin (K)\n",
        "    - RH: Fraction (0-1)\n",
        "    - TSG: Kelvin (K) - ground temperature\n",
        "    - VW: m/s\n",
        "    - DW: Degrees (0-360)\n",
        "    - HS: Meters (m) - snow depth\n",
        "    - ISWR: W/mÂ²\n",
        "    - PSUM: mm\n",
        "\n",
        "    Args:\n",
        "        weather_df: DataFrame containing weather data with timestamp column\n",
        "                   Must have units matching SMET format (see above)\n",
        "        output_path: Path where SMET file will be written\n",
        "        station_id: Station identifier\n",
        "        station_name: Station name\n",
        "        latitude: Latitude in degrees\n",
        "        longitude: Longitude in degrees\n",
        "        altitude_meters: Altitude in meters\n",
        "        timezone: Timezone offset (default: -7 for Mountain Time)\n",
        "    \"\"\"\n",
        "    if \"timestamp\" in weather_df.columns:\n",
        "        weather_df = weather_df.copy()\n",
        "        weather_df[\"timestamp\"] = pd.to_datetime(weather_df[\"timestamp\"])\n",
        "    else:\n",
        "        weather_df = weather_df.copy()\n",
        "        weather_df.reset_index(inplace=True)\n",
        "        weather_df[\"timestamp\"] = pd.to_datetime(weather_df[\"timestamp\"])\n",
        "\n",
        "    fields = [\"timestamp\", \"TA\", \"RH\", \"TSG\", \"VW\", \"DW\", \"ISWR\", \"PSUM\"]\n",
        "    if \"HS\" in weather_df.columns:\n",
        "        fields.append(\"HS\")\n",
        "\n",
        "    output_dir = os.path.dirname(output_path)\n",
        "    if output_dir:\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    with open(output_path, \"w\") as f:\n",
        "        f.write(\"SMET 1.1 ASCII\\n\")\n",
        "        f.write(\"[HEADER]\\n\")\n",
        "        f.write(f\"station_id = {station_id}\\n\")\n",
        "        f.write(f\"station_name = {station_name}\\n\")\n",
        "        f.write(f\"latitude = {latitude:.10f}\\n\")\n",
        "        f.write(f\"longitude = {longitude:.10f}\\n\")\n",
        "        f.write(f\"altitude = {altitude_meters}\\n\")\n",
        "        f.write(\"nodata = -777\\n\")\n",
        "        f.write(f\"Tz = {timezone}\\n\")\n",
        "        f.write(f\"fields = {'\\t'.join(fields)}\\n\")\n",
        "\n",
        "        field_count = len(fields)\n",
        "        units_offset = [\"0\"] * field_count\n",
        "        units_multiplier = [\"1\"] * field_count\n",
        "        f.write(f\"units_offset     = {' '.join(units_offset)}\\n\")\n",
        "        f.write(f\"units_multiplier = {' '.join(units_multiplier)}\\n\")\n",
        "\n",
        "        f.write(\"[DATA]\\n\")\n",
        "        for _, row in weather_df.iterrows():\n",
        "            values = []\n",
        "            for field in fields:\n",
        "                if field == \"timestamp\":\n",
        "                    values.append(row[field].strftime(\"%Y-%m-%dT%H:%M:%S\"))\n",
        "                else:\n",
        "                    val = row.get(field, -777)\n",
        "                    if pd.isna(val):\n",
        "                        val = -777\n",
        "                    values.append(f\"{val:.2f}\")\n",
        "            f.write(\"\\t\".join(values) + \"\\n\")\n",
        "\n",
        "\n",
        "def fetch_openmeteo_historical(lat: float,\n",
        "                               lon: float,\n",
        "                               start_date: str,\n",
        "                               end_date: str,\n",
        "                               model: str = \"gfs_global\",\n",
        "                               elevation: Optional[float] = None) -> Optional[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Fetch historical forecast data from the Open-Meteo API.\n",
        "    \n",
        "    Open-Meteo API Units (as returned):\n",
        "    - temperature_2m: Celsius (Â°C)\n",
        "    - relative_humidity_2m: Percentage (0-100)\n",
        "    - wind_speed_10m: m/s (specified via wind_speed_unit parameter)\n",
        "    - wind_direction_10m: Degrees (0-360)\n",
        "    - snow_depth: Centimeters (cm)\n",
        "    - direct_radiation: W/mÂ²\n",
        "    - precipitation: Millimeters (mm)\n",
        "    \n",
        "    Returns DataFrame with SMET-compatible units:\n",
        "    - TA: Kelvin (K) - converted from Celsius\n",
        "    - RH: Fraction (0-1) - converted from percentage\n",
        "    - VW: m/s - already correct\n",
        "    - DW: Degrees - already correct\n",
        "    - HS: Centimeters (cm) - will be converted to meters later if using model data\n",
        "    - ISWR: W/mÂ² - already correct\n",
        "    - PSUM: mm - already correct\n",
        "    \"\"\"\n",
        "    cache_session = requests_cache.CachedSession(\".cache\", expire_after=3600)\n",
        "    retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
        "    openmeteo = openmeteo_requests.Client(session=retry_session)\n",
        "\n",
        "    url = \"https://historical-forecast-api.open-meteo.com/v1/forecast\"\n",
        "    params = {\n",
        "        \"latitude\": lat,\n",
        "        \"longitude\": lon,\n",
        "        \"start_date\": start_date,\n",
        "        \"end_date\": end_date,\n",
        "        \"hourly\": [\n",
        "            \"temperature_2m\",\n",
        "            \"relative_humidity_2m\",\n",
        "            \"wind_speed_10m\",\n",
        "            \"wind_direction_10m\",\n",
        "            \"snow_depth\",\n",
        "            \"direct_radiation\",\n",
        "            \"precipitation\",\n",
        "        ],\n",
        "        \"models\": model,\n",
        "        \"wind_speed_unit\": \"ms\",\n",
        "    }\n",
        "\n",
        "    if elevation is not None:\n",
        "        params[\"elevation\"] = elevation\n",
        "\n",
        "    print(f\"Fetching {model} data for {lat}, {lon} from {start_date} to {end_date}\")\n",
        "    if \"elevation\" in params:\n",
        "        print(f\"Open-Meteo elevation parameter: {params['elevation']} (m)\")\n",
        "\n",
        "    try:\n",
        "        responses = openmeteo.weather_api(url, params=params)\n",
        "        response = responses[0]\n",
        "\n",
        "        print(f\"Coordinates: {response.Latitude()}Â°N {response.Longitude()}Â°E\")\n",
        "        print(f\"Elevation: {response.Elevation()} m asl\")\n",
        "\n",
        "        hourly = response.Hourly()\n",
        "        hourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\n",
        "        hourly_relative_humidity_2m = hourly.Variables(1).ValuesAsNumpy()\n",
        "        hourly_wind_speed_10m = hourly.Variables(2).ValuesAsNumpy()\n",
        "        hourly_wind_direction_10m = hourly.Variables(3).ValuesAsNumpy()\n",
        "        hourly_snow_depth = hourly.Variables(4).ValuesAsNumpy()\n",
        "        hourly_direct_radiation = hourly.Variables(5).ValuesAsNumpy()\n",
        "        hourly_precipitation = hourly.Variables(6).ValuesAsNumpy()\n",
        "\n",
        "        time_index = pd.date_range(\n",
        "            start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n",
        "            end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n",
        "            freq=pd.Timedelta(seconds=hourly.Interval()),\n",
        "            inclusive=\"left\",\n",
        "        )\n",
        "\n",
        "        # Convert Open-Meteo units to SMET-compatible units\n",
        "        # TA: Open-Meteo returns Celsius, SMET requires Kelvin\n",
        "        ta_kelvin = hourly_temperature_2m + 273.15\n",
        "        \n",
        "        # RH: Open-Meteo returns percentage (0-100), SMET requires fraction (0-1)\n",
        "        rh_fraction = hourly_relative_humidity_2m / 100.0\n",
        "        \n",
        "        # VW: Open-Meteo returns m/s (via wind_speed_unit=\"ms\"), SMET requires m/s - already correct\n",
        "        vw_ms = hourly_wind_speed_10m\n",
        "        \n",
        "        # DW: Open-Meteo returns degrees, SMET requires degrees - already correct\n",
        "        dw_degrees = hourly_wind_direction_10m\n",
        "        \n",
        "        # HS: Open-Meteo returns centimeters (cm), will convert to meters later if using model data\n",
        "        # SNODAS returns meters directly, so we keep Open-Meteo in cm for now\n",
        "        hs_cm = hourly_snow_depth\n",
        "        \n",
        "        # ISWR: Open-Meteo returns W/mÂ², SMET requires W/mÂ² - already correct\n",
        "        iswr_wm2 = hourly_direct_radiation\n",
        "        \n",
        "        # PSUM: Open-Meteo returns mm, SMET requires mm - already correct\n",
        "        psum_mm = hourly_precipitation\n",
        "        \n",
        "        df = pd.DataFrame(\n",
        "            {\n",
        "                \"timestamp\": time_index,\n",
        "                \"TA\": ta_kelvin,           # Units: Kelvin (K)\n",
        "                \"RH\": rh_fraction,         # Units: Fraction (0-1)\n",
        "                \"VW\": vw_ms,               # Units: m/s\n",
        "                \"DW\": dw_degrees,          # Units: Degrees\n",
        "                \"HS\": hs_cm,               # Units: Centimeters (cm) - will convert to meters later\n",
        "                \"ISWR\": iswr_wm2,          # Units: W/mÂ²\n",
        "                \"PSUM\": psum_mm,           # Units: mm\n",
        "            }\n",
        "        )\n",
        "        \n",
        "        # TSG (ground temperature) is not available from Open-Meteo API\n",
        "        # Using assumed value of 273.15 K (0Â°C) as a reasonable default\n",
        "        # SNOWPACK will calculate actual ground temperature during simulation\n",
        "        df[\"TSG\"] = 273.15  # Units: Kelvin (K)\n",
        "        \n",
        "        # Validate temperature range (reasonable bounds: 200-330 K = -73Â°C to 57Â°C)\n",
        "        # Flag extreme values but don't fail - SNOWPACK will handle validation\n",
        "        invalid_ta = (df[\"TA\"] < 200.0) | (df[\"TA\"] > 330.0)\n",
        "        if invalid_ta.any():\n",
        "            invalid_count = invalid_ta.sum()\n",
        "            print(f\"âš  Warning: {invalid_count} temperature values outside reasonable range (200-330 K)\")\n",
        "            print(f\"  Min TA: {df['TA'].min():.2f} K, Max TA: {df['TA'].max():.2f} K\")\n",
        "            print(f\"  These values may cause SNOWPACK to fail. Check input data quality.\")\n",
        "        \n",
        "        # Validate relative humidity range (should be 0-1)\n",
        "        invalid_rh = (df[\"RH\"] < 0.0) | (df[\"RH\"] > 1.0)\n",
        "        if invalid_rh.any():\n",
        "            invalid_count = invalid_rh.sum()\n",
        "            print(f\"âš  Warning: {invalid_count} relative humidity values outside range (0-1)\")\n",
        "            print(f\"  Min RH: {df['RH'].min():.4f}, Max RH: {df['RH'].max():.4f}\")\n",
        "            # Clamp to valid range\n",
        "            df.loc[df[\"RH\"] < 0.0, \"RH\"] = 0.0\n",
        "            df.loc[df[\"RH\"] > 1.0, \"RH\"] = 1.0\n",
        "        \n",
        "        df = df.replace([np.inf, -np.inf], np.nan)\n",
        "        \n",
        "        # Check for NaN values and report\n",
        "        nan_counts = df.isna().sum()\n",
        "        if nan_counts.any():\n",
        "            print(f\"âš  Warning: Found NaN values in data:\")\n",
        "            for col, count in nan_counts.items():\n",
        "                if count > 0:\n",
        "                    print(f\"  {col}: {count} NaN values\")\n",
        "            # Replace NaN with nodata values for SMET compatibility\n",
        "            df = df.fillna(-777)\n",
        "            print(\"  Replaced NaN values with nodata (-777)\")\n",
        "        \n",
        "        print(f\"Fetched {len(df)} records\")\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching data: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# --- Derived values / summary -------------------------------------------\n",
        "\n",
        "# Validate dates\n",
        "try:\n",
        "    from datetime import datetime\n",
        "    start_dt = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
        "    end_dt = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
        "    if start_dt >= end_dt:\n",
        "        raise ValueError(f\"start_date ({start_date}) must be before end_date ({end_date})\")\n",
        "except ValueError as e:\n",
        "    print(f\"Date validation error: {e}\")\n",
        "    print(\"Please check your date inputs and try again.\")\n",
        "    raise\n",
        "\n",
        "# Validate that altitude_meters is available (from configuration cell)\n",
        "try:\n",
        "    altitude_meters\n",
        "except NameError:\n",
        "    print(\"Error: altitude_meters not found. Please run the configuration cell first.\")\n",
        "    raise\n",
        "\n",
        "# altitude_meters = altitude # Commented out direct assignment\n",
        "openmeteo_elevation = resolve_openmeteo_elevation(openmeteo_elevation_mode, altitude_meters)\n",
        "\n",
        "print(\"SMET Generation Parameters\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Location: {latitude}, {longitude}\")\n",
        "print(f\"Altitude input: {altitude} {altitude_unit} ({altitude_meters:.2f} m used for calculations)\")\n",
        "# print(f\"Altitude: {altitude_meters:.2f} m\")\n",
        "print(f\"Station: {station_name}\")\n",
        "print(f\"Period: {start_date} to {end_date}\")\n",
        "print(f\"Model: {model_selection}\")\n",
        "print(f\"HS Source: {hs_source}\")\n",
        "print(f\"Generate: {generate_files}\")\n",
        "if openmeteo_elevation is None:\n",
        "    print(\"Open-Meteo elevation: model elevation (automatic downscaling)\")\n",
        "else:\n",
        "    print(f\"Open-Meteo elevation: {openmeteo_elevation:.2f} m (selected altitude)\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "P_qk-irE3klw",
        "outputId": "5130bc3e-2955-4b27-99ee-f9a5a2cef4c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting SMET generation...\n",
            "Processing 1 model(s): ifs\n",
            "==================================================\n",
            "\n",
            "Processing ifs...\n",
            "Fetching ecmwf_ifs data for 39.71438, -105.84475 from 2024-11-01 to 2025-04-30\n",
            "Open-Meteo elevation parameter: 3596.6400000000003 (m)\n",
            "Coordinates: 39.68365478515625Â°N -105.875Â°E\n",
            "Elevation: 3596.639892578125 m asl\n",
            "Fetched 4344 records\n",
            "SMET file created: /content/input/watrous.smet\n",
            "Records: 4344\n",
            "Period: 2024-11-01 00:00:00+00:00 to 2025-04-30 23:00:00+00:00\n",
            "\n",
            "Successfully generated 1 SMET files:\n",
            "  /content/input/watrous.smet\n",
            "\n",
            "Files saved to: /content/input\n"
          ]
        }
      ],
      "source": [
        "# @title Generate SMET Files\n",
        "# @markdown Run this cell to generate SMET files with the parameters above\n",
        "\n",
        "# Import for parallel processing of cached SNODAS data\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# Hardcoded output path\n",
        "# output_directory = \"/content/input\" # Commented out hardcoded path\n",
        "\n",
        "# Use sno_directory from the configuration cell\n",
        "output_directory = sno_directory  # sno directory is input folder\n",
        "\n",
        "# Model mapping\n",
        "model_mapping = {\n",
        "    \"nbm\": \"ncep_nbm_conus\",\n",
        "    \"ifs\": \"ecmwf_ifs\",\n",
        "    \"gfs\": \"gfs_global\",\n",
        "    \"gfs_hrrr\": \"gfs_hrrr\"\n",
        "}\n",
        "\n",
        "\n",
        "if generate_files:\n",
        "    print(\"Starting SMET generation...\")\n",
        "\n",
        "    # Parse model selection\n",
        "    selected_models = [model_selection]\n",
        "\n",
        "    # Validate and map models\n",
        "    valid_models = ['nbm', 'ifs', 'gfs', 'gfs_hrrr']\n",
        "    selected_models = [model for model in selected_models if model in valid_models]\n",
        "\n",
        "    if not selected_models:\n",
        "        print(\"No valid models selected!\")\n",
        "        print(f\"Available models: {', '.join(valid_models)}\")\n",
        "    else:\n",
        "        print(f\"Processing {len(selected_models)} model(s): {', '.join(selected_models)}\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        successful_files = []\n",
        "\n",
        "        for model in selected_models:\n",
        "            print(f\"\\nProcessing {model}...\")\n",
        "\n",
        "            # Map to Open-Meteo model name\n",
        "            openmeteo_model = model_mapping[model]\n",
        "\n",
        "            # Fetch data\n",
        "            df = fetch_openmeteo_historical(\n",
        "                lat=latitude,\n",
        "                lon=longitude,\n",
        "                start_date=start_date,\n",
        "                end_date=end_date,\n",
        "                model=openmeteo_model,\n",
        "                elevation=openmeteo_elevation\n",
        "            )\n",
        "\n",
        "            if df is None or df.empty:\n",
        "                print(f\"Failed to fetch data for {model}\")\n",
        "                continue\n",
        "\n",
        "            # Initialize SNODAS fallback flag (used later in unit conversion)\n",
        "            snodas_fallback_used = False\n",
        "\n",
        "            # Process SNODAS snow depth data if requested\n",
        "            if hs_source == \"snodas\":\n",
        "                print(f\"\\nProcessing SNODAS snow depth data...\")\n",
        "                \n",
        "                # SNODAS coverage bounds (contiguous US)\n",
        "                SNODAS_YMIN = 24.95\n",
        "                SNODAS_YMAX = 52.83\n",
        "                SNODAS_XMIN = -124.73\n",
        "                SNODAS_XMAX = -66.95\n",
        "                \n",
        "                # Check if location is within SNODAS coverage\n",
        "                if latitude < SNODAS_YMIN or latitude > SNODAS_YMAX or longitude < SNODAS_XMIN or longitude > SNODAS_XMAX:\n",
        "                    print(f\"âš  Location ({latitude}Â°N, {longitude}Â°E) is outside SNODAS coverage area.\")\n",
        "                    print(\"SNODAS data is only available for the contiguous United States.\")\n",
        "                    print(\"All HS values set to nodata (-777). SNOWPACK will interpolate these values during model run.\")\n",
        "                    # Set all HS values to nodata\n",
        "                    df['HS'] = -777\n",
        "                else:\n",
        "                    # Generate date range for SNODAS downloads\n",
        "                    start_dt = pd.to_datetime(start_date)\n",
        "                    end_dt = pd.to_datetime(end_date)\n",
        "                    \n",
        "                    # Create daily SNODAS values dictionary\n",
        "                    snodas_snow_depth = {}\n",
        "                    missing_dates = []\n",
        "                    \n",
        "                    # Generate all dates in range\n",
        "                    all_dates = []\n",
        "                    current_date = start_dt\n",
        "                    while current_date <= end_dt:\n",
        "                        all_dates.append(current_date)\n",
        "                        current_date += timedelta(days=1)\n",
        "                    \n",
        "                    total_dates = len(all_dates)\n",
        "                    print(f\"Processing SNODAS data for {total_dates} days...\")\n",
        "                    print(f\"Location: {latitude}Â°N, {longitude}Â°E\")\n",
        "                    \n",
        "                    # Pre-check cache status\n",
        "                    cache_dir = \"snodas_cache\"\n",
        "                    cached_dates = []\n",
        "                    uncached_dates = []\n",
        "                    for current_date in all_dates:\n",
        "                        date_str = current_date.strftime(\"%Y%m%d\")\n",
        "                        tar_filename = f\"SNODAS_{date_str}.tar\"\n",
        "                        cache_path = os.path.join(cache_dir, tar_filename)\n",
        "                        if os.path.exists(cache_path):\n",
        "                            cached_dates.append(date_str)\n",
        "                        else:\n",
        "                            uncached_dates.append(date_str)\n",
        "                    \n",
        "                    print(f\"  Cache status: {len(cached_dates)} cached, {len(uncached_dates)} need download\")\n",
        "                    \n",
        "                    # Process cached dates in parallel (faster)\n",
        "                    if cached_dates:\n",
        "                        print(f\"\\nProcessing {len(cached_dates)} cached dates in parallel...\")\n",
        "                        with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "                            future_to_date = {\n",
        "                                executor.submit(get_snodas_snow_depth, latitude, longitude, date_str, debug=False): date_str\n",
        "                                for date_str in cached_dates\n",
        "                            }\n",
        "                            for future in as_completed(future_to_date):\n",
        "                                date_str = future_to_date[future]\n",
        "                                try:\n",
        "                                    snow_depth = future.result()\n",
        "                                    if snow_depth is not None:\n",
        "                                        snodas_snow_depth[date_str] = snow_depth\n",
        "                                        print(f\"âœ“ {date_str}: {snow_depth:.3f} m (cached)\")\n",
        "                                    else:\n",
        "                                        missing_dates.append(date_str)\n",
        "                                        print(f\"âœ— {date_str}: No data (cached)\")\n",
        "                                except Exception as e:\n",
        "                                    missing_dates.append(date_str)\n",
        "                                    print(f\"âœ— {date_str}: Error (cached)\")\n",
        "                    \n",
        "                    # Download uncached dates sequentially (to avoid overwhelming server)\n",
        "                    if uncached_dates:\n",
        "                        print(f\"\\nDownloading {len(uncached_dates)} new dates...\")\n",
        "                        for idx, date_str in enumerate(uncached_dates, 1):\n",
        "                            progress = (idx / len(uncached_dates)) * 100\n",
        "                            print(f\"[{idx:3d}/{len(uncached_dates)}] ({progress:5.1f}%) {date_str}... \", end=\"\", flush=True)\n",
        "                            \n",
        "                            try:\n",
        "                                snow_depth = get_snodas_snow_depth(latitude, longitude, date_str, debug=False)\n",
        "                                if snow_depth is not None:\n",
        "                                    snodas_snow_depth[date_str] = snow_depth\n",
        "                                    print(f\"âœ“ {snow_depth:.3f} m\")\n",
        "                                else:\n",
        "                                    missing_dates.append(date_str)\n",
        "                                    print(\"âœ— No data\")\n",
        "                            except Exception as e:\n",
        "                                missing_dates.append(date_str)\n",
        "                                print(\"âœ— Error\")\n",
        "                    \n",
        "                    if len(snodas_snow_depth) == 0:\n",
        "                        print(\"\\nâš  No SNODAS data available for this date range.\")\n",
        "                        print(\"All HS values set to nodata (-777). SNOWPACK will interpolate these values during model run.\")\n",
        "                        df['HS'] = -777\n",
        "                    else:\n",
        "                        print(f\"\\nâœ“ Retrieved SNODAS snow depth for {len(snodas_snow_depth)}/{total_dates} days\")\n",
        "                        if len(missing_dates) > 0:\n",
        "                            print(f\"âš  SNODAS data unavailable for {len(missing_dates)} dates - using nodata (-777).\")\n",
        "                            print(\"SNOWPACK will interpolate these gaps during model run.\")\n",
        "                        \n",
        "                        # Replace model HS values with SNODAS values\n",
        "                        # IMPORTANT UNIT CONVERSION:\n",
        "                        # - df['HS'] is currently in cm (from Open-Meteo)\n",
        "                        # - snodas_snow_depth[date_str] is in meters (from SNODAS function)\n",
        "                        # - We replace cm values with meter values directly\n",
        "                        # - SNODAS provides daily snapshots, apply same value to all hours of each day\n",
        "                        replaced_count = 0\n",
        "                        nodata_count = 0\n",
        "                        \n",
        "                        # Ensure HS column is float64 to avoid dtype warnings\n",
        "                        if df['HS'].dtype != 'float64':\n",
        "                            df['HS'] = df['HS'].astype('float64')\n",
        "                        \n",
        "                        for idx, row in df.iterrows():\n",
        "                            date_str = row['timestamp'].strftime(\"%Y%m%d\")\n",
        "                            if date_str in snodas_snow_depth:\n",
        "                                # Replace Open-Meteo HS (cm) with SNODAS HS (m)\n",
        "                                # snodas_snow_depth[date_str] is already in meters\n",
        "                                df.at[idx, 'HS'] = float(snodas_snow_depth[date_str])  # Units: meters (m)\n",
        "                                replaced_count += 1\n",
        "                            else:\n",
        "                                # Set to nodata for missing dates\n",
        "                                df.at[idx, 'HS'] = -777.0  # Nodata value (float)\n",
        "                                nodata_count += 1\n",
        "                        \n",
        "                        print(f\"âœ“ Applied SNODAS values to {replaced_count} hourly records\")\n",
        "                        if nodata_count > 0:\n",
        "                            print(f\"âœ“ Set {nodata_count} hourly records to nodata (-777) for missing dates\")\n",
        "            \n",
        "            # ====================================================================\n",
        "            # UNIT CONVERSION: Ensure all variables are in SMET-compatible units\n",
        "            # ====================================================================\n",
        "            # SMET Format Requirements:\n",
        "            # - TA: Kelvin (K) âœ“ (already converted from Celsius in fetch function)\n",
        "            # - RH: Fraction (0-1) âœ“ (already converted from percentage in fetch function)\n",
        "            # - VW: m/s âœ“ (already correct from Open-Meteo)\n",
        "            # - DW: Degrees âœ“ (already correct from Open-Meteo)\n",
        "            # - HS: Meters (m) - NEEDS CONVERSION\n",
        "            # - ISWR: W/mÂ² âœ“ (already correct from Open-Meteo)\n",
        "            # - PSUM: mm âœ“ (already correct from Open-Meteo)\n",
        "            # - TSG: Kelvin (K) âœ“ (already set to 273.15 K)\n",
        "            # ====================================================================\n",
        "            \n",
        "            # HS (Snow Depth) Unit Conversion:\n",
        "            # Open-Meteo returns: centimeters (cm)\n",
        "            # SNODAS returns: meters (m)\n",
        "            # SMET requires: meters (m)\n",
        "            \n",
        "            if hs_source == \"model\" and 'HS' in df.columns:\n",
        "                # Using model data: Open-Meteo returns cm, convert to meters for SMET\n",
        "                # df['HS'] is currently in cm from Open-Meteo\n",
        "                df['HS'] = df['HS'] / 100.0  # Convert cm â†’ m\n",
        "                print(f\"\\nâœ“ Converted HS from cm to meters (model data)\")\n",
        "                print(f\"  HS range: {df[df['HS'] >= 0]['HS'].min():.3f} - {df[df['HS'] >= 0]['HS'].max():.3f} m\")\n",
        "            elif hs_source == \"snodas\" and 'HS' in df.columns:\n",
        "                # Check if fallback was used\n",
        "                if snodas_fallback_used:\n",
        "                    # Model data was used (fallback), convert from cm to meters\n",
        "                    # df['HS'] is currently in cm from Open-Meteo\n",
        "                    df['HS'] = df['HS'] / 100.0  # Convert cm â†’ m\n",
        "                    print(\"\\nâš  ALERT: SNODAS was selected but model fallback was used for HS values.\")\n",
        "                    print(f\"  Converted HS from cm to meters (model fallback)\")\n",
        "                    print(f\"  HS range: {df[df['HS'] >= 0]['HS'].min():.3f} - {df[df['HS'] >= 0]['HS'].max():.3f} m\")\n",
        "                else:\n",
        "                    # SNODAS values are already in meters, nodata (-777) values are already set correctly\n",
        "                    # df['HS'] contains SNODAS values in meters where available, -777 for missing dates\n",
        "                    print(f\"\\nâœ“ Using SNODAS HS values (already in meters)\")\n",
        "                    valid_hs = df[df['HS'] != -777]['HS']\n",
        "                    if len(valid_hs) > 0:\n",
        "                        print(f\"  HS range: {valid_hs.min():.3f} - {valid_hs.max():.3f} m\")\n",
        "                    print(f\"  Nodata records: {(df['HS'] == -777).sum()}\")\n",
        "            \n",
        "            # Final validation: Ensure all units are correct before writing SMET\n",
        "            print(f\"\\nðŸ“Š Final data summary (SMET units):\")\n",
        "            \n",
        "            # Check for any problematic values that might cause SNOWPACK to fail\n",
        "            issues_found = []\n",
        "            \n",
        "            # Temperature validation (SNOWPACK typically fails if outside 200-330 K)\n",
        "            ta_min, ta_max = df['TA'].min(), df['TA'].max()\n",
        "            print(f\"  TA: {ta_min:.2f} - {ta_max:.2f} K\")\n",
        "            if ta_min < 200.0 or ta_max > 330.0:\n",
        "                issues_found.append(f\"Temperature out of bounds (200-330 K): {ta_min:.2f} - {ta_max:.2f} K\")\n",
        "            # Check for individual problematic values\n",
        "            extreme_ta = df[(df['TA'] < 200.0) | (df['TA'] > 330.0)]\n",
        "            if len(extreme_ta) > 0:\n",
        "                issues_found.append(f\"{len(extreme_ta)} temperature values outside 200-330 K range\")\n",
        "                print(f\"    âš  Found {len(extreme_ta)} extreme temperature values\")\n",
        "            \n",
        "            # RH validation\n",
        "            rh_min, rh_max = df['RH'].min(), df['RH'].max()\n",
        "            print(f\"  RH: {rh_min:.3f} - {rh_max:.3f} (fraction)\")\n",
        "            if rh_min < 0.0 or rh_max > 1.0:\n",
        "                issues_found.append(f\"Relative humidity out of bounds (0-1): {rh_min:.3f} - {rh_max:.3f}\")\n",
        "            \n",
        "            # VW validation\n",
        "            vw_min, vw_max = df['VW'].min(), df['VW'].max()\n",
        "            print(f\"  VW: {vw_min:.2f} - {vw_max:.2f} m/s\")\n",
        "            if vw_min < 0.0:\n",
        "                issues_found.append(f\"Negative wind speed values found\")\n",
        "            \n",
        "            # HS validation\n",
        "            if 'HS' in df.columns:\n",
        "                valid_hs = df[df['HS'] != -777]['HS']\n",
        "                if len(valid_hs) > 0:\n",
        "                    hs_min, hs_max = valid_hs.min(), valid_hs.max()\n",
        "                    print(f\"  HS: {hs_min:.3f} - {hs_max:.3f} m\")\n",
        "                    if hs_min < 0.0:\n",
        "                        issues_found.append(f\"Negative HS values found\")\n",
        "                print(f\"  HS nodata: {(df['HS'] == -777).sum()} records\")\n",
        "            \n",
        "            # ISWR validation\n",
        "            iswr_min, iswr_max = df['ISWR'].min(), df['ISWR'].max()\n",
        "            print(f\"  ISWR: {iswr_min:.1f} - {iswr_max:.1f} W/mÂ²\")\n",
        "            if iswr_min < 0.0:\n",
        "                issues_found.append(f\"Negative ISWR values found\")\n",
        "            \n",
        "            # PSUM validation\n",
        "            psum_min, psum_max = df['PSUM'].min(), df['PSUM'].max()\n",
        "            print(f\"  PSUM: {psum_min:.2f} - {psum_max:.2f} mm\")\n",
        "            if psum_min < 0.0:\n",
        "                issues_found.append(f\"Negative precipitation values found\")\n",
        "            \n",
        "            # Report any issues\n",
        "            if issues_found:\n",
        "                print(f\"\\nâš  Potential data quality issues detected:\")\n",
        "                for issue in issues_found:\n",
        "                    print(f\"  - {issue}\")\n",
        "                print(f\"  These may cause SNOWPACK to fail. Review the data before proceeding.\")\n",
        "            else:\n",
        "                print(f\"\\nâœ“ All data ranges appear valid for SNOWPACK\")\n",
        "\n",
        "            # Create output filename with simplified naming\n",
        "            output_file = os.path.join(output_directory, f\"{station_id}.smet\")\n",
        "            # could add in _{model} if wanted to specify\n",
        "\n",
        "            try:\n",
        "                # Generate SMET file\n",
        "                create_smet_from_weather_data(\n",
        "                    weather_df=df,\n",
        "                    output_path=output_file,\n",
        "                    station_id=f\"{station_id}_{model}\",\n",
        "                    station_name=f\"{station_name}_{model}\",\n",
        "                    latitude=latitude,\n",
        "                    longitude=longitude,\n",
        "                    altitude_meters=altitude_meters,\n",
        "                    timezone=timezone\n",
        "                )\n",
        "\n",
        "                print(f\"SMET file created: {output_file}\")\n",
        "                print(f\"Records: {len(df)}\")\n",
        "                print(f\"Period: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
        "\n",
        "                successful_files.append(output_file)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error creating SMET for {model}: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"\\nSuccessfully generated {len(successful_files)} SMET files:\")\n",
        "        for file in successful_files:\n",
        "            print(f\"  {file}\")\n",
        "\n",
        "        if successful_files:\n",
        "            print(f\"\\nFiles saved to: {output_directory}\")\n",
        "else:\n",
        "    print(\"Generation disabled. Set 'generate_files' to True to generate SMET files.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KtR-OPRA43OO"
      },
      "outputs": [],
      "source": [
        "# @title Run this cell to run SNOWPACK\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Construct the path to the .ini file using variables from the configuration cell\n",
        "ini_filepath = os.path.join(ini_directory, f\"{station_id}.ini\")\n",
        "\n",
        "# Find the snowpack executable path dynamically\n",
        "try:\n",
        "    result = subprocess.run([\"which\", \"snowpack\"], capture_output=True, text=True, check=True)\n",
        "    snowpack_executable_path = result.stdout.strip()\n",
        "except (subprocess.CalledProcessError, FileNotFoundError):\n",
        "    snowpack_executable_path = \"snowpack\"  # Fallback to PATH lookup\n",
        "\n",
        "# Verify .ini file exists\n",
        "if not os.path.exists(ini_filepath):\n",
        "    raise FileNotFoundError(f\"Configuration file not found: {ini_filepath}\")\n",
        "\n",
        "# Run SNOWPACK with proper error handling and real-time output streaming\n",
        "# In Jupyter notebooks, we need to ensure output streams properly\n",
        "try:\n",
        "    # Use subprocess.Popen with line-by-line streaming to ensure output appears in notebook\n",
        "    import sys\n",
        "    process = subprocess.Popen(\n",
        "        [snowpack_executable_path, \"-c\", ini_filepath, \"-e\", snowpack_end_date],\n",
        "        cwd=ini_directory,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        universal_newlines=True,\n",
        "        bufsize=1  # Line buffered\n",
        "    )\n",
        "    \n",
        "    # Stream output line by line to notebook\n",
        "    for line in process.stdout:\n",
        "        print(line, end='', flush=True)\n",
        "        sys.stdout.flush()\n",
        "    \n",
        "    # Wait for process to complete and get return code\n",
        "    return_code = process.wait()\n",
        "    \n",
        "    if return_code != 0:\n",
        "        raise subprocess.CalledProcessError(return_code, snowpack_executable_path)\n",
        "        \n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"\\nâŒ Error running SNOWPACK (exit code {e.returncode})\")\n",
        "    print(\"Check the output above for detailed error messages.\")\n",
        "    print(\"\\nCommon issues:\")\n",
        "    print(\"  - Temperature values out of bounds (check TA range in data summary)\")\n",
        "    print(\"  - Missing or invalid SMET file\")\n",
        "    print(\"  - Configuration file errors\")\n",
        "    raise\n",
        "except FileNotFoundError:\n",
        "    print(f\"âŒ SNOWPACK executable not found: {snowpack_executable_path}\")\n",
        "    print(\"Please ensure SNOWPACK is compiled and in PATH\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "collapsed": true,
        "id": "2wQuuL4KEJO5",
        "outputId": "0373e4a5-7472-49ad-a943-2466bf22a9cb"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_b8602e68-72cc-4adc-9d7c-c029576ddf78\", \"snowpack_profiles.zip\", 232577340)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "\n",
              "    ---\n",
              "    10 SNOWPACK profile files have been downloaded as **`snowpack_profiles.zip`**.\n",
              "\n",
              "    Next Step â€” View in niViz\n",
              "    1. Go to https://run.niviz.org\n",
              "    2. Click \"File\" â†’ \"Open Profile\" or drag any of the downloaded files into the screen\n",
              "    3. Select any of the downloaded files:\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "   - **`ksp_model_ifs4_res.pro`**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "   - **`ksp_model_ifs1_res.pro`**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "   - **`watrous_ifs3_res.pro`**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "   - **`ksp_model_ifs3_res.pro`**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "   - **`watrous_ifs2_res.pro`**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "   - **`ksp_model_ifs2_res.pro`**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "   - **`ksp_model_ifs_res.pro`**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "   - **`watrous_ifs1_res.pro`**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "   - **`watrous_ifs_res.pro`**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "   - **`watrous_ifs4_res.pro`**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# @title Download .pro files and Open niViz\n",
        "# @markdown # 5 files will be download.\n",
        "# @markdown # 1 = N, 2=E, 3=S, 4=W,\n",
        "import glob, shutil, os\n",
        "from IPython.display import FileLink, Markdown\n",
        "\n",
        "# Check if running in Colab\n",
        "try:\n",
        "    from google.colab import files\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# Determine temp directory based on environment\n",
        "if IN_COLAB:\n",
        "    temp_dir = \"/content\"\n",
        "else:\n",
        "    # For local environments, use current directory or a temp directory\n",
        "    temp_dir = os.getcwd()\n",
        "\n",
        "# Locate all .pro files\n",
        "pro_files = glob.glob(os.path.join(ini_directory, 'output', '*.pro'))\n",
        "\n",
        "if pro_files:\n",
        "    # Copy all files to a simple location\n",
        "    downloaded_files = []\n",
        "    for pro_file in pro_files:\n",
        "        filename = os.path.basename(pro_file)\n",
        "        temp_file_path = os.path.join(temp_dir, filename)\n",
        "        shutil.copy(pro_file, temp_file_path)\n",
        "        downloaded_files.append(filename)\n",
        "\n",
        "    # Create a zip file with all .pro files\n",
        "    import zipfile\n",
        "    zip_filename = 'snowpack_profiles.zip'\n",
        "    zip_path = os.path.join(temp_dir, zip_filename)\n",
        "    with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
        "        for filename in downloaded_files:\n",
        "            file_path = os.path.join(temp_dir, filename)\n",
        "            zipf.write(file_path, filename)\n",
        "\n",
        "    # Download the zip file (only in Colab)\n",
        "    if IN_COLAB:\n",
        "        files.download(zip_path)\n",
        "    else:\n",
        "        print(f\"Zip file created at: {zip_path}\")\n",
        "        print(f\"Files included: {', '.join(downloaded_files)}\")\n",
        "\n",
        "    # Show how to open in niViz\n",
        "    display(Markdown(f\"\"\"\n",
        "    ---\n",
        "    {len(downloaded_files)} SNOWPACK profile files have been downloaded as **`{zip_filename}`**.\n",
        "\n",
        "    Next Step â€” View in niViz\n",
        "    1. Go to https://run.niviz.org\n",
        "    2. Click \"File\" â†’ \"Open Profile\" or drag any of the downloaded files into the screen\n",
        "    3. Select any of the downloaded files:\n",
        "    \"\"\"))\n",
        "\n",
        "    for filename in downloaded_files:\n",
        "        display(Markdown(f\"   - **`{filename}`**\"))\n",
        "\n",
        "else:\n",
        "    display(Markdown(\"No .pro files found in the output directory.\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7EiXf6ZyXoy"
      },
      "source": [
        "# This cell can erase input and ini files to start over.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TfiTcySpenWY"
      },
      "outputs": [],
      "source": [
        "# @title Clean up generated files\n",
        "# @markdown Run this cell to remove generated .sno and .ini files.\n",
        "\n",
        "run_cleanup = False #@param {type:\"boolean\"}\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Use variables from the configuration cell\n",
        "sno_directory_to_clear = sno_directory\n",
        "ini_directory_to_clear = ini_directory\n",
        "\n",
        "def clear_directory(directory_path):\n",
        "    \"\"\"Removes all files and subdirectories within a given directory.\n",
        "\n",
        "    Args:\n",
        "        directory_path: Path to directory to clear\n",
        "\n",
        "    Returns:\n",
        "        tuple: (success: bool, files_deleted: int, errors: list)\n",
        "    \"\"\"\n",
        "    if not os.path.exists(directory_path):\n",
        "        print(f\"Directory not found: {directory_path}\")\n",
        "        return False, 0, []\n",
        "\n",
        "    if not os.path.isdir(directory_path):\n",
        "        print(f\"Error: {directory_path} is not a directory\")\n",
        "        return False, 0, []\n",
        "\n",
        "    # Count files before deletion\n",
        "    file_count = len([f for f in os.listdir(directory_path)\n",
        "                      if os.path.isfile(os.path.join(directory_path, f))])\n",
        "\n",
        "    print(f\"Clearing contents of: {directory_path}\")\n",
        "    print(f\"  Found {file_count} file(s) and {len([d for d in os.listdir(directory_path) if os.path.isdir(os.path.join(directory_path, d))])} subdirectory(ies)\")\n",
        "\n",
        "    files_deleted = 0\n",
        "    errors = []\n",
        "\n",
        "    for item in os.listdir(directory_path):\n",
        "        item_path = os.path.join(directory_path, item)\n",
        "        try:\n",
        "            if os.path.isfile(item_path) or os.path.islink(item_path):\n",
        "                os.unlink(item_path)\n",
        "                files_deleted += 1\n",
        "            elif os.path.isdir(item_path):\n",
        "                shutil.rmtree(item_path)\n",
        "                files_deleted += 1\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Failed to delete {item_path}: {e}\"\n",
        "            errors.append(error_msg)\n",
        "            print(f\"  Warning: {error_msg}\")\n",
        "\n",
        "    if errors:\n",
        "        print(f\"  Completed with {len(errors)} error(s)\")\n",
        "    else:\n",
        "        print(f\"  Successfully deleted {files_deleted} item(s)\")\n",
        "\n",
        "    return len(errors) == 0, files_deleted, errors\n",
        "\n",
        "if run_cleanup:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"WARNING: This will delete all files in the following directories:\")\n",
        "    print(f\"  - {sno_directory_to_clear}\")\n",
        "    print(f\"  - {ini_directory_to_clear}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Clear the input (sno) directory\n",
        "    sno_success, sno_count, sno_errors = clear_directory(sno_directory_to_clear)\n",
        "\n",
        "    # Clear the config (ini) directory\n",
        "    ini_success, ini_count, ini_errors = clear_directory(ini_directory_to_clear)\n",
        "\n",
        "    total_errors = len(sno_errors) + len(ini_errors)\n",
        "    if total_errors == 0:\n",
        "        print(\"\\nâœ“ Cleanup complete successfully.\")\n",
        "    else:\n",
        "        print(f\"\\nâš  Cleanup completed with {total_errors} error(s).\")\n",
        "else:\n",
        "    print(\"Cleanup is disabled. Check the 'Run cleanup' box to enable.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
