{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7887f4ee",
      "metadata": {
        "id": "7887f4ee"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Austfi/SNOWPACKforPatrollers/blob/main/AORC_smet.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "# AORC ‚Üí SMET Converter\n",
        "\n",
        "Hourly workflow that pulls NOAA's AORC 1 km retrospective forcing from the public AWS bucket, blends in SNODAS snow depths for HS, and writes SNOWPACK-ready SMET files with both shortwave (ISWR) and longwave (ILWR) radiation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2edf0637",
      "metadata": {
        "id": "2edf0637"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Austfi/SNOWPACKforPatrollers/blob/main/AORC_smet.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "# AORC ‚Üí SMET Converter\n",
        "\n",
        "Hourly workflow that pulls NOAA's AORC 1 km retrospective forcing from the public AWS bucket, blends in SNODAS snow depths for HS, and writes SNOWPACK-ready SMET files with both shortwave (ISWR) and longwave (ILWR) radiation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7d3e3fe",
      "metadata": {
        "id": "c7d3e3fe"
      },
      "source": [
        "## Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1598e100",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-07T03:43:11.250890Z",
          "iopub.status.busy": "2025-11-07T03:43:11.250566Z",
          "iopub.status.idle": "2025-11-07T03:43:17.220254Z",
          "shell.execute_reply": "2025-11-07T03:43:17.219482Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1598e100",
        "outputId": "fb6ebcf0-c32a-4c5d-8712-bc63e80fdf26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Environment ready - core libraries imported\n"
          ]
        }
      ],
      "source": [
        "# Install packages (if not already installed)\n",
        "%pip install -q s3fs xarray zarr h5netcdf pandas numpy matplotlib tqdm\n",
        "\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "import math\n",
        "import struct\n",
        "import tarfile\n",
        "import gzip\n",
        "import urllib.request\n",
        "import urllib.error\n",
        "from io import BytesIO\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import xarray as xr\n",
        "import s3fs\n",
        "\n",
        "s3_fs = s3fs.S3FileSystem(anon=True)\n",
        "\n",
        "print(\"‚úì Environment ready - core libraries imported\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2537f1fa",
      "metadata": {
        "id": "2537f1fa"
      },
      "source": [
        "## Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d26cf73e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-07T03:43:17.224680Z",
          "iopub.status.busy": "2025-11-07T03:43:17.224152Z",
          "iopub.status.idle": "2025-11-07T03:43:17.235685Z",
          "shell.execute_reply": "2025-11-07T03:43:17.234961Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d26cf73e",
        "outputId": "6e0ed734-aa69-46e3-f041-8658ebf4dd5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Configuration:\n",
            "  Location: 39.5261¬∞N, -106.2131¬∞E\n",
            "  Altitude: 3231.0 m\n",
            "  Station: VAIL_PASS_CO\n",
            "  Period: 2023-10-01 to 2024-04-30 (213 days, 5112 hours)\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# USER INPUT: Edit these values\n",
        "# ============================================\n",
        "\n",
        "latitude = 39.5261    # Vail Pass, Colorado\n",
        "longitude = -106.2131  # Vail Pass, Colorado\n",
        "altitude = 3231.0      # Station altitude in meters\n",
        "station_id = \"VAIL_PASS_CO\"  # Station identifier\n",
        "\n",
        "# Date range (format: YYYY-MM-DD)\n",
        "start_date = os.environ.get(\"AORC_START_DATE\", \"2023-10-01\")\n",
        "end_date = os.environ.get(\"AORC_END_DATE\", \"2024-04-30\")\n",
        "\n",
        "start_dt = pd.to_datetime(start_date)\n",
        "end_dt = pd.to_datetime(end_date)\n",
        "duration_days = (end_dt - start_dt).days + 1\n",
        "hours_requested = duration_days * 24\n",
        "\n",
        "print(f\"‚úì Configuration:\")\n",
        "print(f\"  Location: {latitude}¬∞N, {longitude}¬∞E\")\n",
        "print(f\"  Altitude: {altitude} m\")\n",
        "print(f\"  Station: {station_id}\")\n",
        "print(f\"  Period: {start_date} to {end_date} ({duration_days} days, {hours_requested} hours)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d3bd7e2",
      "metadata": {
        "id": "4d3bd7e2"
      },
      "source": [
        "## AORC Data Utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "170bf6dd",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-07T03:43:17.239505Z",
          "iopub.status.busy": "2025-11-07T03:43:17.239146Z",
          "iopub.status.idle": "2025-11-07T03:43:17.253701Z",
          "shell.execute_reply": "2025-11-07T03:43:17.252944Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "170bf6dd",
        "outputId": "862fbe7f-c1e0-49df-a13f-a9f3b32b1e42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì AORC helper functions ready\n"
          ]
        }
      ],
      "source": [
        "# Helper functions for efficient point sampling of the AORC grid\n",
        "\n",
        "AORC_BUCKET = \"noaa-nws-aorc-v1-1-1km\"\n",
        "AORC_VARIABLES = [\n",
        "    \"TMP_2maboveground\",  # Air temperature (K)\n",
        "    \"SPFH_2maboveground\",  # Specific humidity (kg/kg)\n",
        "    \"UGRD_10maboveground\", # Zonal wind (m/s)\n",
        "    \"VGRD_10maboveground\", # Meridional wind (m/s)\n",
        "    \"DSWRF_surface\",      # Downward shortwave (W/m¬≤)\n",
        "    \"DLWRF_surface\",      # Downward longwave (W/m¬≤)\n",
        "    \"APCP_surface\"        # Accumulated precip (kg/m¬≤ = mm)\n",
        "]\n",
        "PRESSURE_CANDIDATES = [\"PRES_surface\", \"PSFC_surface\", \"PRES\"]\n",
        "\n",
        "def compute_relative_humidity(temp_k, specific_humidity, pressure_pa):\n",
        "    temp_c = temp_k - 273.15\n",
        "    es = 6.112 * np.exp((17.67 * temp_c) / (temp_c + 243.5)) * 100.0\n",
        "    es = np.maximum(es, 1.0)\n",
        "    e = (specific_humidity * pressure_pa) / (0.622 + 0.378 * specific_humidity)\n",
        "    rh = np.clip(e / es, 0.0, 1.0)\n",
        "    return rh\n",
        "\n",
        "def wind_from_components(u, v):\n",
        "    speed = np.sqrt(u ** 2 + v ** 2)\n",
        "    direction = (270.0 - np.degrees(np.arctan2(v, u))) % 360.0\n",
        "    return speed, direction\n",
        "\n",
        "def cumulative_to_hourly(values):\n",
        "    arr = np.asarray(values, dtype=float)\n",
        "    increments = np.diff(arr, prepend=np.nan)\n",
        "    increments[np.isnan(increments)] = arr[0]\n",
        "    resets = increments < 0\n",
        "    increments[resets] = arr[resets]\n",
        "    return np.clip(increments, 0.0, None)\n",
        "\n",
        "def _detect_pressure_var(dataset):\n",
        "    for candidate in PRESSURE_CANDIDATES:\n",
        "        if candidate in dataset.data_vars:\n",
        "            return candidate\n",
        "    return None\n",
        "\n",
        "def fetch_aorc_point_series(lat, lon, start_dt, end_dt, verbose=True):\n",
        "    frames = []\n",
        "    missing_years = []\n",
        "    selected_lat = np.nan\n",
        "    selected_lon = np.nan\n",
        "    pressure_var = None\n",
        "\n",
        "    years = range(start_dt.year, end_dt.year + 1)\n",
        "    for year in years:\n",
        "        year_start = pd.Timestamp(f\"{year}-01-01 00:00:00\")\n",
        "        year_end = pd.Timestamp(f\"{year}-12-31 23:00:00\")\n",
        "        slice_start = max(start_dt, year_start)\n",
        "        slice_end = min(end_dt, year_end)\n",
        "        if slice_start > slice_end:\n",
        "            continue\n",
        "\n",
        "        store_path = f\"{AORC_BUCKET}/{year}.zarr\"\n",
        "        if not s3_fs.exists(store_path):\n",
        "            missing_years.append(year)\n",
        "            if verbose:\n",
        "                print(f\"[{year}] ‚úó Missing store ({store_path})\")\n",
        "            continue\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"[{year}] Loading {store_path} ... \", end=\"\")\n",
        "\n",
        "        mapper = s3_fs.get_mapper(store_path)\n",
        "        ds = xr.open_zarr(mapper, consolidated=True)\n",
        "        try:\n",
        "            if pressure_var is None:\n",
        "                pressure_var = _detect_pressure_var(ds)\n",
        "                if pressure_var is None:\n",
        "                    raise KeyError(\"Surface pressure variable not found in AORC dataset.\")\n",
        "\n",
        "            vars_to_pull = [v for v in AORC_VARIABLES if v in ds.data_vars]\n",
        "            if pressure_var not in vars_to_pull:\n",
        "                vars_to_pull.append(pressure_var)\n",
        "\n",
        "            subset = ds[vars_to_pull].sel(time=slice(slice_start, slice_end))\n",
        "            if subset.time.size == 0:\n",
        "                if verbose:\n",
        "                    print(\"no samples in range\")\n",
        "                continue\n",
        "\n",
        "            point = subset.sel(latitude=lat, longitude=lon, method='nearest').load()\n",
        "            selected_lat = float(point.latitude.values)\n",
        "            selected_lon = float(point.longitude.values)\n",
        "            frames.append(point.to_dataframe().reset_index())\n",
        "            if verbose:\n",
        "                print(f\"‚úì {subset.time.size} hours at {selected_lat:.3f}¬∞, {selected_lon:.3f}¬∞\")\n",
        "        finally:\n",
        "            ds.close()\n",
        "\n",
        "    if not frames:\n",
        "        raise RuntimeError(f\"No AORC data retrieved for {start_dt.date()} to {end_dt.date()}. Missing years: {missing_years or 'none'}\")\n",
        "\n",
        "    raw_df = pd.concat(frames, ignore_index=True)\n",
        "    metadata = {\n",
        "        \"pressure_var\": pressure_var,\n",
        "        \"selected_lat\": selected_lat,\n",
        "        \"selected_lon\": selected_lon,\n",
        "        \"missing_years\": missing_years,\n",
        "        \"total_records\": len(raw_df)\n",
        "    }\n",
        "    return raw_df, metadata\n",
        "\n",
        "print(\"‚úì AORC helper functions ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55c8ff04",
      "metadata": {
        "id": "55c8ff04"
      },
      "source": [
        "## Fetch AORC Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "328c07f3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-07T03:43:17.257793Z",
          "iopub.status.busy": "2025-11-07T03:43:17.257451Z",
          "iopub.status.idle": "2025-11-07T03:43:27.204528Z",
          "shell.execute_reply": "2025-11-07T03:43:27.203675Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "328c07f3",
        "outputId": "9da70fd8-c02a-4bde-86f7-d666760ce033"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching AORC hourly forcing...\n",
            "[2023] Loading noaa-nws-aorc-v1-1-1km/2023.zarr ... ‚úì 2208 hours at 39.524¬∞, -106.209¬∞\n",
            "[2024] Loading noaa-nws-aorc-v1-1-1km/2024.zarr ... ‚úì 2881 hours at 39.524¬∞, -106.209¬∞\n",
            "‚úì Retrieved 5089 hourly records\n",
            "  Selected grid cell: 39.5242¬∞N, -106.2093¬∞E\n",
            "  Pressure field: PRES_surface\n"
          ]
        }
      ],
      "source": [
        "print(\"Fetching AORC hourly forcing...\")\n",
        "raw_df, metadata = fetch_aorc_point_series(latitude, longitude, start_dt, end_dt, verbose=True)\n",
        "\n",
        "raw_df['time'] = pd.to_datetime(raw_df['time'], utc=True)\n",
        "start_ts = start_dt.tz_localize('UTC')\n",
        "end_ts = (end_dt + pd.Timedelta(hours=23)).tz_localize('UTC')\n",
        "\n",
        "pressure = raw_df[metadata['pressure_var']].to_numpy()\n",
        "temperature = raw_df['TMP_2maboveground'].to_numpy()\n",
        "specific_humidity = raw_df['SPFH_2maboveground'].to_numpy()\n",
        "u_wind = raw_df['UGRD_10maboveground'].to_numpy()\n",
        "v_wind = raw_df['VGRD_10maboveground'].to_numpy()\n",
        "shortwave = raw_df['DSWRF_surface'].to_numpy()\n",
        "longwave = raw_df['DLWRF_surface'].to_numpy()\n",
        "precip_cumulative = raw_df['APCP_surface'].to_numpy()\n",
        "\n",
        "relative_humidity = compute_relative_humidity(temperature, specific_humidity, pressure)\n",
        "wind_speed, wind_dir = wind_from_components(u_wind, v_wind)\n",
        "precip_hourly = cumulative_to_hourly(precip_cumulative)\n",
        "\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'timestamp': raw_df['time'],\n",
        "    'temperature_2m': temperature,\n",
        "    'relative_humidity_2m': relative_humidity,\n",
        "    'wind_speed_10m': wind_speed,\n",
        "    'wind_direction_10m': wind_dir,\n",
        "    'shortwave_radiation': shortwave,\n",
        "    'longwave_radiation': longwave,\n",
        "    'precipitation': precip_hourly,\n",
        "})\n",
        "\n",
        "df = df[(df['timestamp'] >= start_ts) & (df['timestamp'] <= end_ts)].sort_values('timestamp').reset_index(drop=True)\n",
        "df['snow_depth'] = np.nan  # will be filled by SNODAS later\n",
        "\n",
        "print(f\"‚úì Retrieved {len(df)} hourly records\")\n",
        "print(f\"  Selected grid cell: {metadata['selected_lat']:.4f}¬∞N, {metadata['selected_lon']:.4f}¬∞E\")\n",
        "print(f\"  Pressure field: {metadata['pressure_var']}\")\n",
        "if metadata['missing_years']:\n",
        "    print(f\"  ‚ö† Missing years (not downloaded): {metadata['missing_years']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f21f615d",
      "metadata": {
        "id": "f21f615d"
      },
      "source": [
        "## SNODAS Snow Depth Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e752589b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-07T03:43:27.208759Z",
          "iopub.status.busy": "2025-11-07T03:43:27.208378Z",
          "iopub.status.idle": "2025-11-07T03:43:27.222619Z",
          "shell.execute_reply": "2025-11-07T03:43:27.221513Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e752589b",
        "outputId": "39c52124-598e-4bee-f95f-964ad621bf27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì SNODAS function ready\n"
          ]
        }
      ],
      "source": [
        "# SNODAS function (simplified - only shows errors when debug=False)\n",
        "\n",
        "def get_snodas_snow_depth(lat, lon, date_str, cache_dir=\"snodas_cache\", debug=False):\n",
        "\n",
        "    \"\"\"Download and extract SNODAS snow depth from NSIDC.\"\"\"\n",
        "\n",
        "    SNODAS_NODATA = -9999\n",
        "\n",
        "    # Grid configurations (detected from file size)\n",
        "    GRID_CONFIGS = {\n",
        "        'old': {'XMIN': -124.73375000000000, 'YMAX': 52.87458333333333,\n",
        "                'XMAX': -66.94208333333333, 'YMIN': 24.94958333333333,\n",
        "                'NCOLS': 6935, 'NROWS': 3351, 'name': 'Pre-Oct-2013'},\n",
        "        'new': {'XMIN': -124.73333333333333, 'YMAX': 52.87500000000000,\n",
        "                'XMAX': -66.94166666666667, 'YMIN': 24.95000000000000,\n",
        "                'NCOLS': 3353, 'NROWS': 3353, 'name': 'Post-Oct-2013'}\n",
        "    }\n",
        "\n",
        "    # Check location bounds\n",
        "    if lat < 24.95 or lat > 52.88 or lon < -124.74 or lon > -66.94:\n",
        "        return None\n",
        "\n",
        "    # Construct URL\n",
        "    tar_filename = f\"SNODAS_{date_str}.tar\"\n",
        "    data_base = \"https://noaadata.apps.nsidc.org/NOAA/G02158/masked\"\n",
        "    year = date_str[:4]\n",
        "    month = date_str[4:6]\n",
        "    month_names = [\"01_Jan\", \"02_Feb\", \"03_Mar\", \"04_Apr\", \"05_May\", \"06_Jun\",\n",
        "                   \"07_Jul\", \"08_Aug\", \"09_Sep\", \"10_Oct\", \"11_Nov\", \"12_Dec\"]\n",
        "    month_dir = month_names[int(month) - 1]\n",
        "    data_url = f\"{data_base}/{year}/{month_dir}/{tar_filename}\"\n",
        "\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    cache_path = os.path.join(cache_dir, tar_filename)\n",
        "\n",
        "    try:\n",
        "        # Download or use cache\n",
        "        if os.path.exists(cache_path):\n",
        "            with open(cache_path, 'rb') as f:\n",
        "                tar_data = BytesIO(f.read())\n",
        "        else:\n",
        "            if debug:\n",
        "                print(f\"  Downloading {date_str}...\")\n",
        "            with urllib.request.urlopen(data_url, timeout=60) as response:\n",
        "                tar_data = BytesIO(response.read())\n",
        "                with open(cache_path, 'wb') as f:\n",
        "                    f.write(tar_data.getvalue())\n",
        "            tar_data.seek(0)\n",
        "\n",
        "        # Extract and decompress\n",
        "        with tarfile.open(fileobj=tar_data, mode='r') as tar:\n",
        "            for member in tar.getmembers():\n",
        "                if '1036' in member.name and member.name.endswith('.dat.gz'):\n",
        "                    snow_depth_gz_file = tar.extractfile(member)\n",
        "                    break\n",
        "            else:\n",
        "                return None\n",
        "\n",
        "        with gzip.open(snow_depth_gz_file, 'rb') as gz_file:\n",
        "            data = gz_file.read()\n",
        "\n",
        "        # Detect grid from file size\n",
        "        num_values = len(data) // 2\n",
        "        grid_config = None\n",
        "        for config in GRID_CONFIGS.values():\n",
        "            if num_values == config['NCOLS'] * config['NROWS']:\n",
        "                grid_config = config\n",
        "                break\n",
        "\n",
        "        if grid_config is None:\n",
        "            return None\n",
        "\n",
        "        # Parse binary data\n",
        "        SNODAS_NCOLS = grid_config['NCOLS']\n",
        "        SNODAS_NROWS = grid_config['NROWS']\n",
        "        values = struct.unpack(f\">{SNODAS_NCOLS * SNODAS_NROWS}h\", data)\n",
        "        snow_depth_array = np.array(values).reshape((SNODAS_NROWS, SNODAS_NCOLS))\n",
        "\n",
        "        # Calculate grid coordinates\n",
        "        SNODAS_XMIN = grid_config['XMIN']\n",
        "        SNODAS_YMAX = grid_config['YMAX']\n",
        "        SNODAS_CELLSIZE_X = (grid_config['XMAX'] - SNODAS_XMIN) / SNODAS_NCOLS\n",
        "        SNODAS_CELLSIZE_Y = (SNODAS_YMAX - grid_config['YMIN']) / SNODAS_NROWS\n",
        "\n",
        "        col = int((lon - SNODAS_XMIN) / SNODAS_CELLSIZE_X)\n",
        "        row = int((SNODAS_YMAX - lat) / SNODAS_CELLSIZE_Y)\n",
        "        col = max(0, min(SNODAS_NCOLS - 1, col))\n",
        "        row = max(0, min(SNODAS_NROWS - 1, row))\n",
        "\n",
        "        # Extract value\n",
        "        snow_depth_raw = snow_depth_array[row, col]\n",
        "        if snow_depth_raw == SNODAS_NODATA or snow_depth_raw < 0:\n",
        "            return None\n",
        "\n",
        "        snow_depth_m = snow_depth_raw / 1000.0\n",
        "        return snow_depth_m if snow_depth_m >= 0.01 else 0.0\n",
        "\n",
        "    except Exception as e:\n",
        "        if debug:\n",
        "            print(f\"  Error: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "print(\"‚úì SNODAS function ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23a634a1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-07T03:43:27.226869Z",
          "iopub.status.busy": "2025-11-07T03:43:27.226515Z",
          "iopub.status.idle": "2025-11-07T03:43:55.927022Z",
          "shell.execute_reply": "2025-11-07T03:43:55.925661Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23a634a1",
        "outputId": "662bc6a0-e082-44f9-d8c9-1140733f267f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading SNODAS snow depth data...\n",
            "Location: 39.5261¬∞N, -106.2131¬∞E | Date range: 2023-10-01 to 2024-04-30\n",
            "Processing 213 days...\n",
            "\n",
            "üîç Testing first date (20231001)...\n",
            "  Downloading 20231001...\n",
            "Result: 0.0\n",
            "\n",
            "[  1/213] (  0.5%) 20231001... ‚úì 0.000 m\n",
            "[  2/213] (  0.9%) 20231002... ‚úì 0.000 m\n",
            "[  3/213] (  1.4%) 20231003... ‚úì 0.021 m\n",
            "[  4/213] (  1.9%) 20231004... ‚úì 0.014 m\n",
            "[  5/213] (  2.3%) 20231005... ‚úì 0.000 m\n",
            "[  6/213] (  2.8%) 20231006... ‚úì 0.000 m\n",
            "[  7/213] (  3.3%) 20231007... ‚úì 0.000 m\n",
            "[  8/213] (  3.8%) 20231008... ‚úì 0.000 m\n",
            "[  9/213] (  4.2%) 20231009... ‚úì 0.000 m\n",
            "[ 10/213] (  4.7%) 20231010... ‚úì 0.000 m\n",
            "[ 11/213] (  5.2%) 20231011... ‚úì 0.000 m\n",
            "[ 12/213] (  5.6%) 20231012... ‚úì 0.047 m\n",
            "[ 13/213] (  6.1%) 20231013... ‚úì 0.223 m\n",
            "[ 14/213] (  6.6%) 20231014... ‚úì 0.191 m\n",
            "[ 15/213] (  7.0%) 20231015... ‚úì 0.170 m\n",
            "[ 16/213] (  7.5%) 20231016... ‚úì 0.140 m\n",
            "[ 17/213] (  8.0%) 20231017... ‚úì 0.048 m\n",
            "[ 18/213] (  8.5%) 20231018... ‚úì 0.000 m\n",
            "[ 19/213] (  8.9%) 20231019... ‚úì 0.000 m\n",
            "[ 20/213] (  9.4%) 20231020... ‚úì 0.000 m\n",
            "[ 21/213] (  9.9%) 20231021... ‚úì 0.000 m\n",
            "[ 22/213] ( 10.3%) 20231022... ‚úì 0.000 m\n",
            "[ 23/213] ( 10.8%) 20231023... ‚úì 0.000 m\n",
            "[ 24/213] ( 11.3%) 20231024... ‚úì 0.000 m\n",
            "[ 25/213] ( 11.7%) 20231025... ‚úì 0.000 m\n",
            "[ 26/213] ( 12.2%) 20231026... ‚úì 0.000 m\n",
            "[ 27/213] ( 12.7%) 20231027... ‚úì 0.017 m\n",
            "[ 28/213] ( 13.1%) 20231028... ‚úì 0.000 m\n",
            "[ 29/213] ( 13.6%) 20231029... ‚úì 0.208 m\n",
            "[ 30/213] ( 14.1%) 20231030... ‚úì 0.202 m\n",
            "[ 31/213] ( 14.6%) 20231031... ‚úì 0.180 m\n",
            "[ 32/213] ( 15.0%) 20231101... ‚úì 0.179 m\n",
            "[ 33/213] ( 15.5%) 20231102... ‚úì 0.162 m\n",
            "[ 34/213] ( 16.0%) 20231103... ‚úì 0.147 m\n",
            "[ 35/213] ( 16.4%) 20231104... ‚úì 0.129 m\n",
            "[ 36/213] ( 16.9%) 20231105... ‚úì 0.121 m\n",
            "[ 37/213] ( 17.4%) 20231106... ‚úì 0.040 m\n",
            "[ 38/213] ( 17.8%) 20231107... ‚úì 0.071 m\n",
            "[ 39/213] ( 18.3%) 20231108... ‚úì 0.037 m\n",
            "[ 40/213] ( 18.8%) 20231109... ‚úì 0.058 m\n",
            "[ 41/213] ( 19.2%) 20231110... ‚úì 0.057 m\n",
            "[ 42/213] ( 19.7%) 20231111... ‚úì 0.054 m\n",
            "[ 43/213] ( 20.2%) 20231112... ‚úì 0.048 m\n",
            "[ 44/213] ( 20.7%) 20231113... ‚úì 0.044 m\n",
            "[ 45/213] ( 21.1%) 20231114... ‚úì 0.038 m\n",
            "[ 46/213] ( 21.6%) 20231115... ‚úì 0.101 m\n",
            "[ 47/213] ( 22.1%) 20231116... ‚úì 0.094 m\n",
            "[ 48/213] ( 22.5%) 20231117... ‚úì 0.108 m\n",
            "[ 49/213] ( 23.0%) 20231118... ‚úì 0.105 m\n",
            "[ 50/213] ( 23.5%) 20231119... ‚úì 0.120 m\n",
            "[ 51/213] ( 23.9%) 20231120... ‚úì 0.151 m\n",
            "[ 52/213] ( 24.4%) 20231121... ‚úì 0.177 m\n",
            "[ 53/213] ( 24.9%) 20231122... ‚úì 0.167 m\n",
            "[ 54/213] ( 25.4%) 20231123... ‚úì 0.156 m\n",
            "[ 55/213] ( 25.8%) 20231124... ‚úì 0.144 m\n",
            "[ 56/213] ( 26.3%) 20231125... ‚úì 0.142 m\n",
            "[ 57/213] ( 26.8%) 20231126... ‚úì 0.147 m\n",
            "[ 58/213] ( 27.2%) 20231127... ‚úì 0.142 m\n",
            "[ 59/213] ( 27.7%) 20231128... ‚úì 0.187 m\n",
            "[ 60/213] ( 28.2%) 20231129... ‚úì 0.179 m\n",
            "[ 61/213] ( 28.6%) 20231130... ‚úì 0.170 m\n",
            "[ 62/213] ( 29.1%) 20231201... ‚úì 0.180 m\n",
            "[ 63/213] ( 29.6%) 20231202... ‚úì 0.240 m\n",
            "[ 64/213] ( 30.0%) 20231203... ‚úì 0.319 m\n",
            "[ 65/213] ( 30.5%) 20231204... ‚úì 0.528 m\n",
            "[ 66/213] ( 31.0%) 20231205... ‚úì 0.489 m\n",
            "[ 67/213] ( 31.5%) 20231206... ‚úì 0.488 m\n",
            "[ 68/213] ( 31.9%) 20231207... ‚úì 0.453 m\n",
            "[ 69/213] ( 32.4%) 20231208... ‚úì 0.425 m\n",
            "[ 70/213] ( 32.9%) 20231209... ‚úì 0.527 m\n",
            "[ 71/213] ( 33.3%) 20231210... ‚úì 0.511 m\n",
            "[ 72/213] ( 33.8%) 20231211... ‚úì 0.485 m\n",
            "[ 73/213] ( 34.3%) 20231212... ‚úì 0.549 m\n",
            "[ 74/213] ( 34.7%) 20231213... "
          ]
        }
      ],
      "source": [
        "# Download SNODAS snow depth data and inject into the forcing dataframe\n",
        "print(\"Downloading SNODAS snow depth data...\")\n",
        "print(f\"Location: {latitude}¬∞N, {longitude}¬∞E | Date range: {start_date} to {end_date}\")\n",
        "\n",
        "snodas_hours_replaced = 0\n",
        "\n",
        "if latitude < 24.95 or latitude > 52.83 or longitude < -124.73 or longitude > -66.95:\n",
        "    print(\"‚ö† Location outside SNODAS coverage. HS will remain zero.\")\n",
        "    use_snodas = False\n",
        "else:\n",
        "    use_snodas = True\n",
        "    snodas_snow_depth = {}\n",
        "    failed_dates = []\n",
        "\n",
        "    all_dates = []\n",
        "    current_date = start_dt\n",
        "    while current_date <= end_dt:\n",
        "        all_dates.append(current_date)\n",
        "        current_date += timedelta(days=1)\n",
        "\n",
        "    total_dates = len(all_dates)\n",
        "    print(f\"Processing {total_dates} days...\")\n",
        "\n",
        "    if len(all_dates) > 0:\n",
        "        test_date_str = all_dates[0].strftime(\"%Y%m%d\")\n",
        "        print(f\"\\nüîç Testing first date ({test_date_str})...\")\n",
        "        test_result = get_snodas_snow_depth(latitude, longitude, test_date_str, debug=True)\n",
        "        print(f\"Result: {test_result}\\n\")\n",
        "\n",
        "    for idx, current_date in enumerate(all_dates, 1):\n",
        "        date_str = current_date.strftime(\"%Y%m%d\")\n",
        "        progress = (idx / total_dates) * 100\n",
        "\n",
        "        print(f\"[{idx:3d}/{total_dates}] ({progress:5.1f}%) {date_str}... \", end=\"\", flush=True)\n",
        "\n",
        "        try:\n",
        "            snow_depth = get_snodas_snow_depth(latitude, longitude, date_str, debug=False)\n",
        "            if snow_depth is not None:\n",
        "                snodas_snow_depth[date_str] = snow_depth\n",
        "                print(f\"‚úì {snow_depth:.3f} m\")\n",
        "            else:\n",
        "                failed_dates.append(date_str)\n",
        "                print(\"‚úó No data\")\n",
        "        except Exception:\n",
        "            failed_dates.append(date_str)\n",
        "            print(\"‚úó Error\")\n",
        "\n",
        "    if len(snodas_snow_depth) == 0:\n",
        "        print(\"\\n‚ö† No SNODAS data available. HS will stay at zero.\")\n",
        "        use_snodas = False\n",
        "    else:\n",
        "        print(f\"\\n‚úì Retrieved SNODAS snow depth for {len(snodas_snow_depth)}/{total_dates} days\")\n",
        "        if len(failed_dates) > 0:\n",
        "            print(f\"‚ö† {len(failed_dates)} dates failed\")\n",
        "\n",
        "        replaced_count = 0\n",
        "        for idx, row in df.iterrows():\n",
        "            date_str = row['timestamp'].strftime(\"%Y%m%d\")\n",
        "            if date_str in snodas_snow_depth:\n",
        "                df.at[idx, 'snow_depth'] = snodas_snow_depth[date_str] * 1000.0  # mm\n",
        "                replaced_count += 1\n",
        "        snodas_hours_replaced = replaced_count\n",
        "        print(f\"‚úì Replaced HS values for {replaced_count} hourly records\")\n",
        "\n",
        "if not use_snodas:\n",
        "    df['snow_depth'] = df['snow_depth'].fillna(0.0)\n",
        "    print(\"\\nNote: HS filled with zeros because SNODAS data was unavailable.\")\n",
        "else:\n",
        "    remaining = df['snow_depth'].isna().sum()\n",
        "    if remaining > 0:\n",
        "        df['snow_depth'] = df['snow_depth'].fillna(0.0)\n",
        "        print(f\"‚ö† {remaining} hours missing SNODAS values were filled with zeros.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6aeac2d7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-07T03:43:55.933136Z",
          "iopub.status.busy": "2025-11-07T03:43:55.932644Z",
          "iopub.status.idle": "2025-11-07T03:43:56.006109Z",
          "shell.execute_reply": "2025-11-07T03:43:56.005317Z"
        },
        "id": "6aeac2d7"
      },
      "outputs": [],
      "source": [
        "# Convert to SMET format with ILWR included\n",
        "print(\"Converting data to SMET format...\")\n",
        "\n",
        "smet_df = pd.DataFrame()\n",
        "smet_df['timestamp'] = df['timestamp'].dt.strftime('%Y-%m-%dT%H:%M:%S')\n",
        "smet_df['TA'] = df['temperature_2m']\n",
        "smet_df['RH'] = df['relative_humidity_2m']\n",
        "smet_df['VW'] = df['wind_speed_10m']\n",
        "smet_df['DW'] = df['wind_direction_10m']\n",
        "\n",
        "if use_snodas and snodas_hours_replaced > 0:\n",
        "    smet_df['HS'] = df['snow_depth'] / 1000.0\n",
        "    print(f\"  ‚úì Using SNODAS snow depth ({snodas_hours_replaced} hourly samples)\")\n",
        "else:\n",
        "    smet_df['HS'] = df['snow_depth'].fillna(0.0) / 1000.0\n",
        "    print(\"  ‚ö† SNODAS unavailable - HS values are zeros\")\n",
        "\n",
        "smet_df['ISWR'] = df['shortwave_radiation']\n",
        "smet_df['ILWR'] = df['longwave_radiation']\n",
        "smet_df['PSUM'] = df['precipitation']\n",
        "\n",
        "nodata_value = -999\n",
        "smet_df = smet_df.fillna(nodata_value)\n",
        "\n",
        "numeric_cols = ['TA', 'RH', 'VW', 'DW', 'HS', 'ISWR', 'ILWR', 'PSUM']\n",
        "for col in numeric_cols:\n",
        "    smet_df[col] = pd.to_numeric(smet_df[col], errors='coerce').fillna(nodata_value)\n",
        "\n",
        "print(f\"‚úì Data converted to SMET format\")\n",
        "print(f\"  Records: {len(smet_df)}\")\n",
        "\n",
        "fields = ['timestamp', 'TA', 'RH', 'VW', 'DW', 'HS', 'ISWR', 'ILWR', 'PSUM']\n",
        "smet_filename = f\"{station_id}_aorc_{start_date}_{end_date}.smet\"\n",
        "\n",
        "with open(smet_filename, 'w') as f:\n",
        "    f.write('SMET 1.2 ASCII\\n')\n",
        "    f.write('[HEADER]\\n')\n",
        "    f.write(f\"station_id = {station_id}\\n\")\n",
        "    f.write(f\"latitude = {latitude:.10f}\\n\")\n",
        "    f.write(f\"longitude = {longitude:.10f}\\n\")\n",
        "    f.write(f\"altitude = {altitude:.2f}\\n\")\n",
        "    f.write(f\"nodata = {nodata_value}\\n\")\n",
        "    f.write('tz = 0\\n')\n",
        "    f.write(f\"fields = {' '.join(fields)}\\n\")\n",
        "    f.write('[DATA]\\n')\n",
        "\n",
        "    for _, row in smet_df.iterrows():\n",
        "        values = []\n",
        "        for field in fields:\n",
        "            val = row[field]\n",
        "            if field == 'timestamp':\n",
        "                values.append(str(val))\n",
        "            else:\n",
        "                values.append(f\"{nodata_value}\" if pd.isna(val) or val == nodata_value else f\"{val:.2f}\")\n",
        "        f.write('\\t'.join(values) + '\\n')\n",
        "\n",
        "file_size = os.path.getsize(smet_filename)\n",
        "print(f\"\\n‚úì SMET file written: {smet_filename}\")\n",
        "print(f\"  File size: {file_size/1024:.2f} KB\")\n",
        "print(f\"\\nFirst few records:\")\n",
        "print(smet_df.head())\n",
        "print(f\"\\nData summary:\")\n",
        "print(smet_df[numeric_cols].describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84f16e1c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-07T03:43:56.009607Z",
          "iopub.status.busy": "2025-11-07T03:43:56.009255Z",
          "iopub.status.idle": "2025-11-07T03:43:57.126850Z",
          "shell.execute_reply": "2025-11-07T03:43:57.126048Z"
        },
        "id": "84f16e1c"
      },
      "outputs": [],
      "source": [
        "# Primary time series plots\n",
        "plot_df = smet_df.copy()\n",
        "plot_df['timestamp'] = pd.to_datetime(plot_df['timestamp'])\n",
        "\n",
        "fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n",
        "\n",
        "ax1 = axes[0]\n",
        "ax1.plot(plot_df['timestamp'], plot_df['TA'] - 273.15, color='red', linewidth=1.5, label='Temperature')\n",
        "ax1.set_ylabel('Temperature (¬∞C)', fontsize=12)\n",
        "ax1.set_title(f'AORC Forcing Time Series - {station_id}', fontsize=14, fontweight='bold')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.legend(loc='upper left')\n",
        "\n",
        "ax2 = axes[1]\n",
        "ax2.plot(plot_df['timestamp'], plot_df['HS'], color='blue', linewidth=1.5, label='SNODAS HS', alpha=0.8)\n",
        "ax2.set_ylabel('Snow Depth (m)', fontsize=12)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.legend(loc='upper left')\n",
        "\n",
        "ax3 = axes[2]\n",
        "ax3.bar(plot_df['timestamp'], plot_df['PSUM'], color='green', alpha=0.7, width=0.03, label='Hourly PSUM')\n",
        "ax3.set_ylabel('Precipitation (mm)', fontsize=12)\n",
        "ax3.set_xlabel('Time', fontsize=12)\n",
        "ax3.grid(True, alpha=0.3, axis='y')\n",
        "ax3.legend(loc='upper left')\n",
        "\n",
        "plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('‚úì Time series plots created')\n",
        "print(f\"  Snow depth range: {plot_df['HS'].min():.3f} - {plot_df['HS'].max():.3f} m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09cc6992",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-07T03:43:57.132877Z",
          "iopub.status.busy": "2025-11-07T03:43:57.132315Z",
          "iopub.status.idle": "2025-11-07T03:43:58.293620Z",
          "shell.execute_reply": "2025-11-07T03:43:58.292978Z"
        },
        "id": "09cc6992"
      },
      "outputs": [],
      "source": [
        "# Additional diagnostics\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10), sharex=True)\n",
        "\n",
        "ax1 = axes[0, 0]\n",
        "ax1.plot(plot_df['timestamp'], plot_df['VW'], color='purple', linewidth=1.5, label='Wind Speed')\n",
        "ax1.set_ylabel('Wind Speed (m/s)', fontsize=11)\n",
        "ax1.set_title('Wind Speed', fontsize=12)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.legend()\n",
        "\n",
        "ax2 = axes[0, 1]\n",
        "ax2.plot(plot_df['timestamp'], plot_df['DW'], color='orange', linewidth=1.2, label='Wind Direction')\n",
        "ax2.set_ylabel('Wind Direction (¬∞)', fontsize=11)\n",
        "ax2.set_title('Wind Direction', fontsize=12)\n",
        "ax2.set_ylim(0, 360)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.legend()\n",
        "\n",
        "ax3 = axes[1, 0]\n",
        "ax3.plot(plot_df['timestamp'], plot_df['RH'] * 100, color='teal', linewidth=1.5, label='Relative Humidity')\n",
        "ax3.set_ylabel('Relative Humidity (%)', fontsize=11)\n",
        "ax3.set_title('Relative Humidity', fontsize=12)\n",
        "ax3.set_ylim(0, 100)\n",
        "ax3.grid(True, alpha=0.3)\n",
        "ax3.legend()\n",
        "\n",
        "ax4 = axes[1, 1]\n",
        "ax4.plot(plot_df['timestamp'], plot_df['ISWR'], color='gold', linewidth=1.2, label='ISWR')\n",
        "ax4.plot(plot_df['timestamp'], plot_df['ILWR'], color='brown', linewidth=1.2, label='ILWR')\n",
        "ax4.set_ylabel('Radiation (W/m¬≤)', fontsize=11)\n",
        "ax4.set_title('Shortwave vs Longwave', fontsize=12)\n",
        "ax4.set_xlabel('Time', fontsize=11)\n",
        "ax4.grid(True, alpha=0.3)\n",
        "ax4.legend()\n",
        "\n",
        "plt.setp(ax4.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('‚úì Additional plots created')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdf2d66a",
      "metadata": {
        "id": "fdf2d66a"
      },
      "source": [
        "## Summary\n",
        "\n",
        "- Fields in SMET: timestamp, TA, RH, VW, DW, HS (SNODAS), ISWR, ILWR, PSUM\n",
        "- HS (snow depth) sourced from SNODAS daily tarballs and converted to meters\n",
        "- PSUM represents hourly precipitation increments derived from the cumulative AORC field\n",
        "- ILWR (downward longwave) is now written alongside ISWR for full-radiation forcing"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}