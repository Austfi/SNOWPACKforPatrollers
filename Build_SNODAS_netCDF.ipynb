{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/Austfi/SNOWPACKforPatrollers/blob/feature/snodas_nc/Build_SNODAS_netCDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SNODAS netCDF Dataset Builder (Colorado)\n",
        "\n",
        "This notebook downloads SNODAS (Snow Data Assimilation System) data and builds reusable netCDF files organized by year, subset to Colorado boundaries. These files can be referenced by all other notebooks for fast point queries without re-downloading data.\n",
        "\n",
        "**Features:**\n",
        "- Downloads SNODAS data from NSIDC\n",
        "- Extracts ALL available variables from tar files (snow depth, SWE, accumulation, melt, etc.)\n",
        "- Subsets to Colorado mountain region (37-41°N, -109 to -104°W)\n",
        "- Builds one netCDF file per year (e.g., `snodas_co_2024.nc`)\n",
        "- Incremental updates: only downloads missing dates\n",
        "- Handles grid configuration changes (pre/post Oct 2013)\n",
        "- Compressed storage for efficient access\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xarray in /opt/anaconda3/lib/python3.12/site-packages (2025.9.0)\n",
            "Requirement already satisfied: netcdf4 in /opt/anaconda3/lib/python3.12/site-packages (1.7.2)\n",
            "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (1.26.4)\n",
            "Requirement already satisfied: packaging>=24.1 in /opt/anaconda3/lib/python3.12/site-packages (from xarray) (24.1)\n",
            "Requirement already satisfied: cftime in /opt/anaconda3/lib/python3.12/site-packages (from netcdf4) (1.6.4.post1)\n",
            "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from netcdf4) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "✓ Libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "# @title Install and Import Required Libraries\n",
        "%pip install xarray netcdf4 pandas numpy\n",
        "\n",
        "import os\n",
        "import struct\n",
        "import tarfile\n",
        "import gzip\n",
        "import urllib.request\n",
        "import urllib.error\n",
        "import re\n",
        "from io import BytesIO\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "\n",
        "print(\"✓ Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration:\n",
            "  Years: 2024 to 2024\n",
            "  Output directory: snodas_nc\n",
            "  Cache directory: snodas_cache\n",
            "  Rebuild existing: False\n"
          ]
        }
      ],
      "source": [
        "# @title Configuration Parameters\n",
        "\n",
        "# @markdown ### Date Range\n",
        "start_year = 2020  # @param {type:\"integer\"}\n",
        "end_year = 2025  # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown ### Directories\n",
        "output_directory = \"snodas_nc\"  # @param {type:\"string\"}\n",
        "cache_directory = \"snodas_cache\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown ### Options\n",
        "subset_to_colorado = True  # @param {type:\"boolean\"}\n",
        "rebuild_existing = False  # @param {type:\"boolean\"}\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"  Years: {start_year} to {end_year}\")\n",
        "print(f\"  Output directory: {output_directory}\")\n",
        "print(f\"  Cache directory: {cache_directory}\")\n",
        "print(f\"  Subset to Colorado: {subset_to_colorado}\")\n",
        "print(f\"  Rebuild existing: {rebuild_existing}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Core functions loaded\n"
          ]
        }
      ],
      "source": [
        "# Core Functions for SNODAS Data Processing\n",
        "\n",
        "SNODAS_NODATA = -9999\n",
        "\n",
        "# Colorado bounds (mountain region, excluding plains)\n",
        "CO_BOUNDS = {\n",
        "    'lat_min': 37.0,\n",
        "    'lat_max': 41.0,\n",
        "    'lon_min': -109.0,  # West boundary\n",
        "    'lon_max': -104.0   # East boundary (excluding plains)\n",
        "}\n",
        "\n",
        "# SNODAS product code to variable name mapping\n",
        "# Common SNODAS variables found in tar files\n",
        "VAR_NAMES = {\n",
        "    '1036': 'snow_depth',        # Snow Depth (mm -> m)\n",
        "    '1034': 'swe',               # Snow Water Equivalent (mm -> m)\n",
        "    '1038': 'snow_accumulation', # Snow Depth Accumulation (mm -> m)\n",
        "    '1039': 'snow_melt',         # Snow Melt (mm -> m)\n",
        "    '1033': 'snow_cover',        # Snow Cover (percentage)\n",
        "    '1037': 'snow_depth_change', # Snow Depth Change (mm -> m)\n",
        "}\n",
        "\n",
        "# Grid configurations (detected from file size)\n",
        "GRID_CONFIGS = {\n",
        "    'old': {\n",
        "        'XMIN': -124.73375000000000,\n",
        "        'YMAX': 52.87458333333333,\n",
        "        'XMAX': -66.94208333333333,\n",
        "        'YMIN': 24.94958333333333,\n",
        "        'NCOLS': 6935,\n",
        "        'NROWS': 3351,\n",
        "        'name': 'Pre-Oct-2013'\n",
        "    },\n",
        "    'new': {\n",
        "        'XMIN': -124.73333333333333,\n",
        "        'YMAX': 52.87500000000000,\n",
        "        'XMAX': -66.94166666666667,\n",
        "        'YMIN': 24.95000000000000,\n",
        "        'NCOLS': 3353,\n",
        "        'NROWS': 3353,\n",
        "        'name': 'Post-Oct-2013'\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "def subset_to_colorado(lat_array, lon_array, data_array):\n",
        "    \"\"\"\n",
        "    Subset SNODAS grid to Colorado mountain region.\n",
        "    \n",
        "    Args:\n",
        "        lat_array: 1D latitude array\n",
        "        lon_array: 1D longitude array\n",
        "        data_array: 2D data array (NROWS x NCOLS) or 3D (time, NROWS, NCOLS)\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (lat_subset, lon_subset, data_subset)\n",
        "    \"\"\"\n",
        "    # Find indices within Colorado bounds\n",
        "    lat_mask = (lat_array >= CO_BOUNDS['lat_min']) & (lat_array <= CO_BOUNDS['lat_max'])\n",
        "    lon_mask = (lon_array >= CO_BOUNDS['lon_min']) & (lon_array <= CO_BOUNDS['lon_max'])\n",
        "    \n",
        "    # Get indices\n",
        "    lat_indices = np.where(lat_mask)[0]\n",
        "    lon_indices = np.where(lon_mask)[0]\n",
        "    \n",
        "    # Subset coordinate arrays\n",
        "    lat_subset = lat_array[lat_indices]\n",
        "    lon_subset = lon_array[lon_indices]\n",
        "    \n",
        "    # Subset data (rows = lat, cols = lon)\n",
        "    if data_array.ndim == 2:\n",
        "        # 2D array: (NROWS, NCOLS)\n",
        "        data_subset = data_array[np.ix_(lat_indices, lon_indices)]\n",
        "    elif data_array.ndim == 3:\n",
        "        # 3D array: (time, NROWS, NCOLS)\n",
        "        # First subset rows (latitude), then columns (longitude)\n",
        "        data_subset = data_array[:, lat_indices, :][:, :, lon_indices]\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported array dimension: {data_array.ndim}\")\n",
        "    \n",
        "    return lat_subset, lon_subset, data_subset\n",
        "\n",
        "\n",
        "def get_snodas_grid(date_str: str, cache_dir: str, subset_co: bool = True, verbose: bool = False) -> Optional[Tuple[dict, dict]]:\n",
        "    \"\"\"\n",
        "    Extract all available SNODAS variables for a specific date.\n",
        "    \n",
        "    Args:\n",
        "        date_str: Date string in YYYYMMDD format\n",
        "        cache_dir: Directory to cache tar files\n",
        "        subset_co: If True, subset to Colorado bounds\n",
        "        verbose: Print progress messages\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (variables_dict, grid_info) or None if failed\n",
        "        - variables_dict: Dictionary with variable names as keys and 2D arrays as values (in meters)\n",
        "        - grid_info: Dictionary with grid metadata and coordinate arrays\n",
        "    \"\"\"\n",
        "    # Construct URL\n",
        "    tar_filename = f\"SNODAS_{date_str}.tar\"\n",
        "    data_base = \"https://noaadata.apps.nsidc.org/NOAA/G02158/masked\"\n",
        "    year = date_str[:4]\n",
        "    month = date_str[4:6]\n",
        "    month_names = [\"01_Jan\", \"02_Feb\", \"03_Mar\", \"04_Apr\", \"05_May\", \"06_Jun\",\n",
        "                   \"07_Jul\", \"08_Aug\", \"09_Sep\", \"10_Oct\", \"11_Nov\", \"12_Dec\"]\n",
        "    month_dir = month_names[int(month) - 1]\n",
        "    data_url = f\"{data_base}/{year}/{month_dir}/{tar_filename}\"\n",
        "    \n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    cache_path = os.path.join(cache_dir, tar_filename)\n",
        "    \n",
        "    try:\n",
        "        # Download or use cache\n",
        "        if os.path.exists(cache_path):\n",
        "            if verbose:\n",
        "                print(f\"    Using cached: {date_str}\")\n",
        "            with open(cache_path, 'rb') as f:\n",
        "                tar_data = BytesIO(f.read())\n",
        "        else:\n",
        "            if verbose:\n",
        "                print(f\"    Downloading: {date_str}\")\n",
        "            with urllib.request.urlopen(data_url, timeout=60) as response:\n",
        "                tar_data = BytesIO(response.read())\n",
        "                with open(cache_path, 'wb') as f:\n",
        "                    f.write(tar_data.getvalue())\n",
        "            tar_data.seek(0)\n",
        "        \n",
        "        # Extract all available variables from tar file\n",
        "        variables_dict = {}\n",
        "        grid_config = None\n",
        "        grid_info = None\n",
        "        \n",
        "        with tarfile.open(fileobj=tar_data, mode='r') as tar:\n",
        "            # Get all .dat.gz files in the tar\n",
        "            for member in tar.getmembers():\n",
        "                if member.name.endswith('.dat.gz'):\n",
        "                    # Extract product code from filename (e.g., us_ssmv11036tS...1036...)\n",
        "                    # Look for 4-digit codes in the filename\n",
        "                    codes = re.findall(r'(\\d{4})', member.name)\n",
        "                    \n",
        "                    # Find matching product code\n",
        "                    var_code = None\n",
        "                    for code in codes:\n",
        "                        if code in VAR_NAMES:\n",
        "                            var_code = code\n",
        "                            break\n",
        "                    \n",
        "                    if var_code is None:\n",
        "                        # Try to extract from common patterns\n",
        "                        if '1036' in member.name:\n",
        "                            var_code = '1036'\n",
        "                        elif '1034' in member.name:\n",
        "                            var_code = '1034'\n",
        "                        elif '1038' in member.name:\n",
        "                            var_code = '1038'\n",
        "                        elif '1039' in member.name:\n",
        "                            var_code = '1039'\n",
        "                        elif '1033' in member.name:\n",
        "                            var_code = '1033'\n",
        "                        elif '1037' in member.name:\n",
        "                            var_code = '1037'\n",
        "                        else:\n",
        "                            continue  # Skip unknown variables\n",
        "                    \n",
        "                    var_name = VAR_NAMES.get(var_code, f'var_{var_code}')\n",
        "                    \n",
        "                    # Extract and decompress\n",
        "                    var_file = tar.extractfile(member)\n",
        "                    with gzip.open(var_file, 'rb') as gz_file:\n",
        "                        data = gz_file.read()\n",
        "                    \n",
        "                    # Detect grid from file size (only need to do once)\n",
        "                    if grid_config is None:\n",
        "                        num_values = len(data) // 2\n",
        "                        for config in GRID_CONFIGS.values():\n",
        "                            if num_values == config['NCOLS'] * config['NROWS']:\n",
        "                                grid_config = config\n",
        "                                break\n",
        "                        \n",
        "                        if grid_config is None:\n",
        "                            if verbose:\n",
        "                                print(f\"    Error: Unknown grid size for {date_str}\")\n",
        "                            return None\n",
        "                        \n",
        "                        # Create coordinate arrays (only once)\n",
        "                        XMIN = grid_config['XMIN']\n",
        "                        YMAX = grid_config['YMAX']\n",
        "                        XMAX = grid_config['XMAX']\n",
        "                        YMIN = grid_config['YMIN']\n",
        "                        NCOLS = grid_config['NCOLS']\n",
        "                        NROWS = grid_config['NROWS']\n",
        "                        CELLSIZE_X = (XMAX - XMIN) / NCOLS\n",
        "                        CELLSIZE_Y = (YMAX - YMIN) / NROWS\n",
        "                        \n",
        "                        lon = np.linspace(XMIN + CELLSIZE_X/2, XMAX - CELLSIZE_X/2, NCOLS)\n",
        "                        lat = np.linspace(YMAX - CELLSIZE_Y/2, YMIN + CELLSIZE_Y/2, NROWS)\n",
        "                    \n",
        "                    # Parse binary data\n",
        "                    NCOLS = grid_config['NCOLS']\n",
        "                    NROWS = grid_config['NROWS']\n",
        "                    values = struct.unpack(f\">{NCOLS * NROWS}h\", data)\n",
        "                    var_array = np.array(values, dtype=np.float32).reshape((NROWS, NCOLS))\n",
        "                    \n",
        "                    # Convert to meters and handle nodata\n",
        "                    # SNODAS stores values in mm, convert to m\n",
        "                    var_array = var_array.astype(np.float32) / 1000.0\n",
        "                    var_array[var_array < 0.01] = 0.0\n",
        "                    var_array[var_array == SNODAS_NODATA / 1000.0] = np.nan\n",
        "                    \n",
        "                    # Subset to Colorado if requested\n",
        "                    if subset_co:\n",
        "                        lat_sub, lon_sub, var_array = subset_to_colorado(lat, lon, var_array)\n",
        "                        if grid_info is None:\n",
        "                            # Store subset coordinates\n",
        "                            grid_info = {\n",
        "                                'lat': lat_sub,\n",
        "                                'lon': lon_sub,\n",
        "                                'config': grid_config,\n",
        "                                'date': date_str,\n",
        "                                'subset_co': True\n",
        "                            }\n",
        "                    else:\n",
        "                        if grid_info is None:\n",
        "                            grid_info = {\n",
        "                                'lat': lat,\n",
        "                                'lon': lon,\n",
        "                                'config': grid_config,\n",
        "                                'date': date_str,\n",
        "                                'subset_co': False\n",
        "                            }\n",
        "                    \n",
        "                    variables_dict[var_name] = var_array\n",
        "        \n",
        "        if not variables_dict:\n",
        "            if verbose:\n",
        "                print(f\"    Error: No variables found in {date_str}\")\n",
        "            return None\n",
        "        \n",
        "        return variables_dict, grid_info\n",
        "        \n",
        "    except Exception as e:\n",
        "        if verbose:\n",
        "            print(f\"    Error processing {date_str}: {e}\")\n",
        "        import traceback\n",
        "        if verbose:\n",
        "            traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "\n",
        "def build_year_dataset(year: int, output_dir: str, cache_dir: str, \n",
        "                      subset_co: bool = True, rebuild: bool = False, \n",
        "                      verbose: bool = True) -> bool:\n",
        "    \"\"\"\n",
        "    Build or update netCDF dataset for a single year with all available variables.\n",
        "    \n",
        "    Args:\n",
        "        year: Year to process (e.g., 2024)\n",
        "        output_dir: Directory to save netCDF files\n",
        "        cache_dir: Directory for cached tar files\n",
        "        subset_co: If True, subset to Colorado bounds\n",
        "        rebuild: If True, rebuild even if file exists\n",
        "        verbose: Print progress messages\n",
        "    \n",
        "    Returns:\n",
        "        True if successful, False otherwise\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # Update filename based on subset\n",
        "    if subset_co:\n",
        "        output_file = os.path.join(output_dir, f\"snodas_co_{year}.nc\")\n",
        "    else:\n",
        "        output_file = os.path.join(output_dir, f\"snodas_{year}.nc\")\n",
        "    \n",
        "    # Generate all dates for the year\n",
        "    start_date = pd.Timestamp(f\"{year}-01-01\")\n",
        "    # For current year, only go up to today\n",
        "    if year == pd.Timestamp.now().year:\n",
        "        end_date = pd.Timestamp.now().date()\n",
        "    else:\n",
        "        end_date = pd.Timestamp(f\"{year}-12-31\").date()\n",
        "    \n",
        "    all_dates = pd.date_range(start_date, end_date, freq='D')\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"\\nProcessing year {year} ({len(all_dates)} days)...\")\n",
        "        if subset_co:\n",
        "            print(f\"  Subsetting to Colorado: {CO_BOUNDS['lat_min']}-{CO_BOUNDS['lat_max']}°N, {CO_BOUNDS['lon_min']}-{CO_BOUNDS['lon_max']}°W\")\n",
        "    \n",
        "    # Check if file exists and get existing dates\n",
        "    existing_dates = set()\n",
        "    if os.path.exists(output_file) and not rebuild:\n",
        "        try:\n",
        "            ds_existing = xr.open_dataset(output_file)\n",
        "            existing_dates = set(pd.to_datetime(ds_existing.time.values).strftime('%Y%m%d'))\n",
        "            ds_existing.close()\n",
        "            if verbose:\n",
        "                print(f\"  Found existing file with {len(existing_dates)} dates\")\n",
        "        except Exception as e:\n",
        "            if verbose:\n",
        "                print(f\"  Warning: Could not read existing file: {e}\")\n",
        "    \n",
        "    # Determine which dates to process\n",
        "    dates_to_process = []\n",
        "    for date in all_dates:\n",
        "        date_str = date.strftime('%Y%m%d')\n",
        "        if date_str not in existing_dates:\n",
        "            dates_to_process.append(date_str)\n",
        "    \n",
        "    if not dates_to_process:\n",
        "        if verbose:\n",
        "            print(f\"  ✓ Year {year} is complete. No new dates to process.\")\n",
        "        return True\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"  Processing {len(dates_to_process)} new/missing dates...\")\n",
        "    \n",
        "    # Collect grids for all variables\n",
        "    all_variables = {}  # {var_name: [grid1, grid2, ...]}\n",
        "    dates = []\n",
        "    grid_info = None\n",
        "    failed_dates = []\n",
        "    \n",
        "    for idx, date_str in enumerate(dates_to_process, 1):\n",
        "        if verbose and idx % 30 == 0:\n",
        "            print(f\"    Progress: {idx}/{len(dates_to_process)} ({100*idx/len(dates_to_process):.1f}%)\")\n",
        "        \n",
        "        result = get_snodas_grid(date_str, cache_dir, subset_co=subset_co, verbose=False)\n",
        "        if result is not None:\n",
        "            var_dict, info = result\n",
        "            dates.append(pd.to_datetime(date_str, format='%Y%m%d'))\n",
        "            \n",
        "            # Store grids for each variable\n",
        "            for var_name, grid in var_dict.items():\n",
        "                if var_name not in all_variables:\n",
        "                    all_variables[var_name] = []\n",
        "                all_variables[var_name].append(grid)\n",
        "            \n",
        "            if grid_info is None:\n",
        "                grid_info = info\n",
        "        else:\n",
        "            failed_dates.append(date_str)\n",
        "    \n",
        "    if not all_variables:\n",
        "        if verbose:\n",
        "            print(f\"  ✗ No data retrieved for year {year}\")\n",
        "        return False\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"  ✓ Retrieved {len(dates)}/{len(dates_to_process)} dates\")\n",
        "        print(f\"  Variables found: {', '.join(sorted(all_variables.keys()))}\")\n",
        "        if failed_dates:\n",
        "            print(f\"  ⚠ Failed dates: {len(failed_dates)}\")\n",
        "    \n",
        "    # Stack into 3D arrays for each variable\n",
        "    lat = grid_info['lat']\n",
        "    lon = grid_info['lon']\n",
        "    \n",
        "    # Create data_vars dictionary for xarray Dataset\n",
        "    data_vars = {}\n",
        "    for var_name, grids in all_variables.items():\n",
        "        var_3d = np.stack(grids, axis=0)\n",
        "        data_vars[var_name] = (['time', 'latitude', 'longitude'], var_3d)\n",
        "    \n",
        "    # Create xarray Dataset with all variables\n",
        "    ds_new = xr.Dataset(\n",
        "        data_vars,\n",
        "        coords={\n",
        "            'time': pd.to_datetime(dates),\n",
        "            'latitude': lat,\n",
        "            'longitude': lon,\n",
        "        },\n",
        "        attrs={\n",
        "            'title': f'SNODAS Dataset - {year}' + (' (Colorado)' if subset_co else ''),\n",
        "            'source': 'NOAA NSIDC SNODAS',\n",
        "            'units': 'meters',\n",
        "            'nodata': 'NaN',\n",
        "            'grid_config': grid_info['config']['name'],\n",
        "            'variables': ', '.join(sorted(all_variables.keys())),\n",
        "            'colorado_bounds': str(CO_BOUNDS) if subset_co else 'full_conus'\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    # Merge with existing data if updating\n",
        "    if os.path.exists(output_file) and not rebuild and existing_dates:\n",
        "        try:\n",
        "            ds_existing = xr.open_dataset(output_file)\n",
        "            # Combine datasets, ensuring time dimension is sorted\n",
        "            ds_combined = xr.concat([ds_existing, ds_new], dim='time')\n",
        "            ds_combined = ds_combined.sortby('time')\n",
        "            ds_existing.close()\n",
        "            ds_new = ds_combined\n",
        "            if verbose:\n",
        "                print(f\"  ✓ Merged with existing data\")\n",
        "        except Exception as e:\n",
        "            if verbose:\n",
        "                print(f\"  Warning: Could not merge with existing file: {e}\")\n",
        "            # Continue with new data only\n",
        "    \n",
        "    # Save to netCDF with compression for all variables\n",
        "    encoding = {}\n",
        "    for var_name in all_variables.keys():\n",
        "        encoding[var_name] = {\n",
        "            'zlib': True,\n",
        "            'complevel': 4,\n",
        "            'dtype': 'float32'\n",
        "        }\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"  Saving to {output_file}...\")\n",
        "    \n",
        "    ds_new.to_netcdf(output_file, encoding=encoding)\n",
        "    \n",
        "    if verbose:\n",
        "        file_size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
        "        print(f\"  ✓ Saved: {file_size_mb:.2f} MB\")\n",
        "        print(f\"    Dimensions: {ds_new.dims}\")\n",
        "        print(f\"    Variables: {', '.join(sorted(all_variables.keys()))}\")\n",
        "        print(f\"    Time range: {ds_new.time.min().values} to {ds_new.time.max().values}\")\n",
        "    \n",
        "    return True\n",
        "\n",
        "\n",
        "print(\"✓ Core functions loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "SNODAS netCDF Dataset Builder\n",
            "============================================================\n",
            "\n",
            "Processing years 2024 to 2024\n",
            "Output directory: snodas_nc\n",
            "Cache directory: snodas_cache\n",
            "Rebuild existing: False\n",
            "\n",
            "============================================================\n",
            "\n",
            "Processing year 2024 (366 days)...\n",
            "  Processing 366 new/missing dates...\n",
            "    Progress: 30/366 (8.2%)\n",
            "    Progress: 60/366 (16.4%)\n",
            "    Progress: 90/366 (24.6%)\n",
            "    Progress: 120/366 (32.8%)\n",
            "    Progress: 150/366 (41.0%)\n",
            "    Progress: 180/366 (49.2%)\n",
            "    Progress: 210/366 (57.4%)\n",
            "    Progress: 240/366 (65.6%)\n",
            "    Progress: 270/366 (73.8%)\n",
            "    Progress: 300/366 (82.0%)\n",
            "    Progress: 330/366 (90.2%)\n",
            "    Progress: 360/366 (98.4%)\n",
            "  ✓ Retrieved 366/366 dates\n",
            "  Saving to snodas_nc/snodas_2024.nc...\n",
            "  ✓ Saved: 1478.74 MB\n",
            "    Dimensions: FrozenMappingWarningOnValuesAccess({'time': 366, 'latitude': 3351, 'longitude': 6935})\n",
            "    Time range: 2024-01-01T00:00:00.000000000 to 2024-12-31T00:00:00.000000000\n",
            "\n",
            "============================================================\n",
            "Summary\n",
            "============================================================\n",
            "✓ Successful years: 1\n",
            "  [2024]\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# @title Build SNODAS netCDF Files\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"SNODAS netCDF Dataset Builder\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Validate year range\n",
        "if start_year > end_year:\n",
        "    raise ValueError(f\"start_year ({start_year}) must be <= end_year ({end_year})\")\n",
        "\n",
        "current_year = pd.Timestamp.now().year\n",
        "if end_year > current_year:\n",
        "    print(f\"⚠ Warning: end_year ({end_year}) is in the future. Clamping to {current_year}.\")\n",
        "    end_year = current_year\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "os.makedirs(cache_directory, exist_ok=True)\n",
        "\n",
        "print(f\"\\nProcessing years {start_year} to {end_year}\")\n",
        "print(f\"Output directory: {output_directory}\")\n",
        "print(f\"Cache directory: {cache_directory}\")\n",
        "print(f\"Rebuild existing: {rebuild_existing}\")\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "# Process each year\n",
        "successful_years = []\n",
        "failed_years = []\n",
        "\n",
        "for year in range(start_year, end_year + 1):\n",
        "    try:\n",
        "        success = build_year_dataset(\n",
        "            year=year,\n",
        "            output_dir=output_directory,\n",
        "            cache_dir=cache_directory,\n",
        "            subset_co=subset_to_colorado,\n",
        "            rebuild=rebuild_existing,\n",
        "            verbose=True\n",
        "        )\n",
        "        if success:\n",
        "            successful_years.append(year)\n",
        "        else:\n",
        "            failed_years.append(year)\n",
        "    except Exception as e:\n",
        "        print(f\"\\n✗ Error processing year {year}: {e}\")\n",
        "        failed_years.append(year)\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Summary\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"✓ Successful years: {len(successful_years)}\")\n",
        "if successful_years:\n",
        "    print(f\"  {successful_years}\")\n",
        "if failed_years:\n",
        "    print(f\"✗ Failed years: {len(failed_years)}\")\n",
        "    print(f\"  {failed_years}\")\n",
        "print(\"\\n\" + \"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Created Files\n",
            "============================================================\n",
            "Found 1 netCDF file(s):\n",
            "\n",
            "  snodas_2024.nc\n",
            "    Size: 1478.74 MB\n",
            "    Dates: 366 (2024-01-01T00:00:00.000000000 to 2024-12-31T00:00:00.000000000)\n",
            "    Dimensions: FrozenMappingWarningOnValuesAccess({'time': 366, 'latitude': 3351, 'longitude': 6935})\n",
            "\n",
            "Total size: 1.44 GB\n",
            "\n",
            "============================================================\n",
            "Example Usage in Other Notebooks\n",
            "============================================================\n",
            "\n",
            "# Load a specific year's dataset\n",
            "import xarray as xr\n",
            "ds = xr.open_dataset('snodas_nc/snodas_2024.nc')\n",
            "\n",
            "# Query a point value for a specific date\n",
            "latitude = 39.5261  # Example: Vail Pass, Colorado\n",
            "longitude = -106.2131\n",
            "date_str = '2024-01-15'\n",
            "\n",
            "point_value = ds.sel(\n",
            "    time=date_str,\n",
            "    latitude=latitude,\n",
            "    longitude=longitude,\n",
            "    method='nearest'\n",
            ").snow_depth.values\n",
            "\n",
            "print(f\"Snow depth on {date_str}: {point_value:.3f} m\")\n",
            "\n",
            "# Get time series for a point\n",
            "point_series = ds.sel(\n",
            "    latitude=latitude,\n",
            "    longitude=longitude,\n",
            "    method='nearest'\n",
            ").snow_depth\n",
            "\n",
            "# Convert to pandas Series for easy plotting\n",
            "snow_depth_ts = point_series.to_pandas()\n",
            "\n",
            "# Close dataset when done\n",
            "ds.close()\n",
            "\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# @title Verify Created Files and Example Usage\n",
        "\n",
        "import glob\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Created Files\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# List all netCDF files (both full and Colorado subset)\n",
        "nc_files = sorted(glob.glob(os.path.join(output_directory, \"snodas*.nc\")))\n",
        "\n",
        "if not nc_files:\n",
        "    print(\"No netCDF files found.\")\n",
        "else:\n",
        "    print(f\"Found {len(nc_files)} netCDF file(s):\\n\")\n",
        "    \n",
        "    total_size = 0\n",
        "    for nc_file in nc_files:\n",
        "        file_size = os.path.getsize(nc_file)\n",
        "        file_size_mb = file_size / (1024 * 1024)\n",
        "        total_size += file_size\n",
        "        filename = os.path.basename(nc_file)\n",
        "        \n",
        "        # Try to get dataset info\n",
        "        try:\n",
        "            ds = xr.open_dataset(nc_file)\n",
        "            time_range = f\"{ds.time.min().values} to {ds.time.max().values}\"\n",
        "            num_dates = len(ds.time)\n",
        "            dims_info = ds.dims\n",
        "            ds.close()\n",
        "            print(f\"  {filename}\")\n",
        "            print(f\"    Size: {file_size_mb:.2f} MB\")\n",
        "            print(f\"    Dates: {num_dates} ({time_range})\")\n",
        "            print(f\"    Dimensions: {dims_info}\")\n",
        "            print()\n",
        "        except Exception as e:\n",
        "            print(f\"  {filename}\")\n",
        "            print(f\"    Size: {file_size_mb:.2f} MB\")\n",
        "            print(f\"    Error reading: {e}\")\n",
        "            print()\n",
        "    \n",
        "    total_size_gb = total_size / (1024 * 1024 * 1024)\n",
        "    print(f\"Total size: {total_size_gb:.2f} GB\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Example Usage in Other Notebooks\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\"\"\n",
        "# Load a specific year's dataset (Colorado subset)\n",
        "import xarray as xr\n",
        "ds = xr.open_dataset('snodas_nc/snodas_co_2024.nc')\n",
        "\n",
        "# Query a point value for a specific date\n",
        "latitude = 39.5261  # Example: Vail Pass, Colorado\n",
        "longitude = -106.2131\n",
        "date_str = '2024-01-15'\n",
        "\n",
        "# Access snow depth\n",
        "point_depth = ds.sel(\n",
        "    time=date_str,\n",
        "    latitude=latitude,\n",
        "    longitude=longitude,\n",
        "    method='nearest'\n",
        ").snow_depth.values\n",
        "\n",
        "# Access SWE (if available)\n",
        "if 'swe' in ds.data_vars:\n",
        "    point_swe = ds.sel(\n",
        "        time=date_str,\n",
        "        latitude=latitude,\n",
        "        longitude=longitude,\n",
        "        method='nearest'\n",
        "    ).swe.values\n",
        "    print(f\"SWE on {date_str}: {point_swe:.3f} m\")\n",
        "\n",
        "print(f\"Snow depth on {date_str}: {point_depth:.3f} m\")\n",
        "\n",
        "# Get time series for a point (all variables)\n",
        "point_data = ds.sel(\n",
        "    latitude=latitude,\n",
        "    longitude=longitude,\n",
        "    method='nearest'\n",
        ")\n",
        "\n",
        "# Convert to pandas DataFrame for easy plotting\n",
        "df = point_data.to_dataframe()\n",
        "\n",
        "# Close dataset when done\n",
        "ds.close()\n",
        "\"\"\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
