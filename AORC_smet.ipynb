{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7887f4ee",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Austfi/SNOWPACKforPatrollers/blob/main/AORC_smet.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# AORC â†’ SMET Converter\n",
    "\n",
    "Hourly workflow that pulls NOAA's AORC 1 km retrospective forcing from the public AWS bucket, blends in SNODAS snow depths for HS, and writes SNOWPACK-ready SMET files with both shortwave (ISWR) and longwave (ILWR) radiation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d3e3fe",
   "metadata": {},
   "source": [
    "## Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1598e100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages (if not already installed)\n",
    "%pip install -q s3fs xarray h5netcdf pandas numpy matplotlib tqdm\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import math\n",
    "import struct\n",
    "import tarfile\n",
    "import gzip\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "from io import BytesIO\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import s3fs\n",
    "\n",
    "s3_fs = s3fs.S3FileSystem(anon=True)\n",
    "\n",
    "print(\"âœ“ Environment ready - core libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2537f1fa",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26cf73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# USER INPUT: Edit these values\n",
    "# ============================================\n",
    "\n",
    "latitude = 39.5261    # Vail Pass, Colorado\n",
    "longitude = -106.2131  # Vail Pass, Colorado\n",
    "altitude = 3231.0      # Station altitude in meters\n",
    "station_id = \"VAIL_PASS_CO\"  # Station identifier\n",
    "\n",
    "# Date range (format: YYYY-MM-DD)\n",
    "start_date = \"2023-11-01\"\n",
    "end_date = \"2024-04-10\"\n",
    "\n",
    "start_dt = pd.to_datetime(start_date)\n",
    "end_dt = pd.to_datetime(end_date)\n",
    "duration_days = (end_dt - start_dt).days + 1\n",
    "hours_requested = duration_days * 24\n",
    "\n",
    "print(f\"âœ“ Configuration:\")\n",
    "print(f\"  Location: {latitude}Â°N, {longitude}Â°E\")\n",
    "print(f\"  Altitude: {altitude} m\")\n",
    "print(f\"  Station: {station_id}\")\n",
    "print(f\"  Period: {start_date} to {end_date} ({duration_days} days, {hours_requested} hours)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3bd7e2",
   "metadata": {},
   "source": [
    "## AORC Data Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170bf6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for efficient point sampling of the AORC grid\n",
    "\n",
    "AORC_BUCKET = \"noaa-nws-aorc-v1-1-1km\"\n",
    "AORC_VARIABLES = [\n",
    "    \"TMP_2maboveground\",  # Air temperature (K)\n",
    "    \"SPFH_2maboveground\",  # Specific humidity (kg/kg)\n",
    "    \"UGRD_10maboveground\", # Zonal wind (m/s)\n",
    "    \"VGRD_10maboveground\", # Meridional wind (m/s)\n",
    "    \"DSWRF_surface\",      # Downward shortwave (W/mÂ²)\n",
    "    \"DLWRF_surface\",      # Downward longwave (W/mÂ²)\n",
    "    \"APCP_surface\"        # Accumulated precip (kg/mÂ² = mm)\n",
    "]\n",
    "PRESSURE_CANDIDATES = [\"PRES_surface\", \"PSFC_surface\", \"PRES\"]\n",
    "LAT_CANDIDATES = [\"lat\", \"latitude\", \"Latitude\", \"LAT\"]\n",
    "LON_CANDIDATES = [\"lon\", \"longitude\", \"Longitude\", \"LON\"]\n",
    "\n",
    "\n",
    "def build_aorc_path(date_obj):\n",
    "    return f\"s3://{AORC_BUCKET}/Y{date_obj:%Y}/M{date_obj:%m}/D{date_obj:%d}/AORC_{date_obj:%Y%m%d}_01.nc\"\n",
    "\n",
    "\n",
    "def wrap_longitude_to_domain(value, lon_min, lon_max):\n",
    "    lon = float(value)\n",
    "    if np.isnan(lon_min) or np.isnan(lon_max):\n",
    "        return lon\n",
    "    span = lon_max - lon_min\n",
    "    if span >= 359.0:\n",
    "        return ((lon - lon_min) % 360.0) + lon_min\n",
    "    while lon < lon_min:\n",
    "        lon += 360.0\n",
    "    while lon > lon_max:\n",
    "        lon -= 360.0\n",
    "    return lon\n",
    "\n",
    "\n",
    "def get_coord_var(dataset, candidates):\n",
    "    for name in candidates:\n",
    "        if name in dataset.coords:\n",
    "            return dataset.coords[name]\n",
    "        if name in dataset.data_vars:\n",
    "            return dataset[name]\n",
    "    return None\n",
    "\n",
    "\n",
    "def locate_grid_cell(dataset, target_lat, target_lon):\n",
    "    lat_var = get_coord_var(dataset, LAT_CANDIDATES)\n",
    "    lon_var = get_coord_var(dataset, LON_CANDIDATES)\n",
    "    if lat_var is None or lon_var is None:\n",
    "        raise KeyError(\"Latitude/longitude coordinates not found in AORC file.\")\n",
    "\n",
    "    lat_vals = lat_var.values\n",
    "    lon_vals = lon_var.values\n",
    "    lon_adj = wrap_longitude_to_domain(target_lon, np.nanmin(lon_vals), np.nanmax(lon_vals))\n",
    "\n",
    "    if lat_vals.ndim == 1 and lon_vals.ndim == 1:\n",
    "        lat_idx = int(np.nanargmin(np.abs(lat_vals - target_lat)))\n",
    "        lon_idx = int(np.nanargmin(np.abs(lon_vals - lon_adj)))\n",
    "        grid_index = {lat_var.dims[0]: lat_idx, lon_var.dims[0]: lon_idx}\n",
    "        selected = (float(lat_vals[lat_idx]), float(lon_vals[lon_idx]))\n",
    "        return grid_index, selected\n",
    "\n",
    "    if lat_vals.ndim >= 2 and lon_vals.ndim >= 2:\n",
    "        if lon_vals.shape != lat_vals.shape:\n",
    "            lon_vals = np.broadcast_to(lon_vals, lat_vals.shape)\n",
    "        distance = np.sqrt((lat_vals - target_lat) ** 2 + (lon_vals - lon_adj) ** 2)\n",
    "        if np.isnan(distance).all():\n",
    "            raise ValueError(\"Unable to locate valid lat/lon grid cell (all NaNs).\")\n",
    "        flat_index = int(np.nanargmin(distance))\n",
    "        idx = np.unravel_index(flat_index, distance.shape)\n",
    "        lat_dims = lat_var.dims[-2:]\n",
    "        grid_index = {lat_dims[0]: idx[0], lat_dims[1]: idx[1]}\n",
    "        selected = (float(lat_vals[idx]), float(lon_vals[idx]))\n",
    "        return grid_index, selected\n",
    "\n",
    "    raise ValueError(\"Unexpected latitude/longitude dimensions.\")\n",
    "\n",
    "\n",
    "def compute_relative_humidity(temp_k, specific_humidity, pressure_pa):\n",
    "    temp_c = temp_k - 273.15\n",
    "    es = 6.112 * np.exp((17.67 * temp_c) / (temp_c + 243.5)) * 100.0\n",
    "    es = np.maximum(es, 1.0)\n",
    "    e = (specific_humidity * pressure_pa) / (0.622 + 0.378 * specific_humidity)\n",
    "    rh = np.clip(e / es, 0.0, 1.0)\n",
    "    return rh\n",
    "\n",
    "\n",
    "def wind_from_components(u, v):\n",
    "    speed = np.sqrt(u ** 2 + v ** 2)\n",
    "    direction = (270.0 - np.degrees(np.arctan2(v, u))) % 360.0\n",
    "    return speed, direction\n",
    "\n",
    "\n",
    "def cumulative_to_hourly(values):\n",
    "    arr = np.asarray(values, dtype=float)\n",
    "    increments = np.diff(arr, prepend=np.nan)\n",
    "    increments[np.isnan(increments)] = arr[0]\n",
    "    resets = increments < 0\n",
    "    increments[resets] = arr[resets]\n",
    "    return np.clip(increments, 0.0, None)\n",
    "\n",
    "\n",
    "def fetch_aorc_point_series(lat, lon, start_dt, end_dt, verbose=True):\n",
    "    frames = []\n",
    "    missing_days = []\n",
    "    grid_index = None\n",
    "    selected_coords = None\n",
    "    pressure_var = None\n",
    "\n",
    "    daily_dates = pd.date_range(start_dt.normalize(), end_dt.normalize(), freq='D')\n",
    "    total_days = len(daily_dates)\n",
    "\n",
    "    for idx, current_date in enumerate(daily_dates, 1):\n",
    "        path = build_aorc_path(current_date)\n",
    "        prefix = f\"[{idx:3d}/{total_days:3d}] {current_date:%Y-%m-%d} â†’ \"\n",
    "        if verbose:\n",
    "            print(prefix, end=\"\")\n",
    "        try:\n",
    "            with s3_fs.open(path, mode='rb') as fobj:\n",
    "                with xr.open_dataset(fobj, engine='h5netcdf', mask_and_scale=True, decode_times=True) as ds:\n",
    "                    if grid_index is None:\n",
    "                        grid_index, selected_coords = locate_grid_cell(ds, lat, lon)\n",
    "                        if verbose:\n",
    "                            print(f\"grid {grid_index} \", end=\"\")\n",
    "                    if pressure_var is None:\n",
    "                        for candidate in PRESSURE_CANDIDATES:\n",
    "                            if candidate in ds.data_vars:\n",
    "                                pressure_var = candidate\n",
    "                                break\n",
    "                        if pressure_var is None:\n",
    "                            raise KeyError(\"No surface pressure variable found in file.\")\n",
    "                    vars_to_pull = [v for v in AORC_VARIABLES if v in ds.data_vars]\n",
    "                    if pressure_var not in vars_to_pull:\n",
    "                        vars_to_pull.append(pressure_var)\n",
    "                    point_ds = ds[vars_to_pull].isel(**grid_index).load()\n",
    "        except FileNotFoundError:\n",
    "            missing_days.append(current_date)\n",
    "            if verbose:\n",
    "                print(\"âœ— missing file\")\n",
    "            continue\n",
    "        except Exception as exc:\n",
    "            missing_days.append(current_date)\n",
    "            if verbose:\n",
    "                print(f\"âœ— {exc}\")\n",
    "            continue\n",
    "\n",
    "        frames.append(point_ds.to_dataframe().reset_index())\n",
    "        if verbose:\n",
    "            print(\"âœ“\")\n",
    "\n",
    "    if not frames:\n",
    "        raise RuntimeError(\"No AORC files were downloaded; please verify the date range and AWS access.\")\n",
    "\n",
    "    raw_df = pd.concat(frames, ignore_index=True)\n",
    "    metadata = {\n",
    "        \"pressure_var\": pressure_var,\n",
    "        \"selected_lat\": selected_coords[0] if selected_coords else np.nan,\n",
    "        \"selected_lon\": selected_coords[1] if selected_coords else np.nan,\n",
    "        \"missing_days\": [d.strftime(\"%Y-%m-%d\") for d in missing_days],\n",
    "        \"total_records\": len(raw_df)\n",
    "    }\n",
    "    return raw_df, metadata\n",
    "\n",
    "\n",
    "print(\"âœ“ AORC helper functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c8ff04",
   "metadata": {},
   "source": [
    "## Fetch AORC Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328c07f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fetching AORC hourly forcing...\")\n",
    "raw_df, metadata = fetch_aorc_point_series(latitude, longitude, start_dt, end_dt, verbose=True)\n",
    "\n",
    "raw_df['time'] = pd.to_datetime(raw_df['time'], utc=True)\n",
    "start_ts = start_dt.tz_localize('UTC')\n",
    "end_ts = (end_dt + pd.Timedelta(hours=23)).tz_localize('UTC')\n",
    "\n",
    "pressure = raw_df[metadata['pressure_var']].to_numpy()\n",
    "temperature = raw_df['TMP_2maboveground'].to_numpy()\n",
    "specific_humidity = raw_df['SPFH_2maboveground'].to_numpy()\n",
    "u_wind = raw_df['UGRD_10maboveground'].to_numpy()\n",
    "v_wind = raw_df['VGRD_10maboveground'].to_numpy()\n",
    "shortwave = raw_df['DSWRF_surface'].to_numpy()\n",
    "longwave = raw_df['DLWRF_surface'].to_numpy()\n",
    "precip_cumulative = raw_df['APCP_surface'].to_numpy()\n",
    "\n",
    "relative_humidity = compute_relative_humidity(temperature, specific_humidity, pressure)\n",
    "wind_speed, wind_dir = wind_from_components(u_wind, v_wind)\n",
    "precip_hourly = cumulative_to_hourly(precip_cumulative)\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'timestamp': raw_df['time'],\n",
    "    'temperature_2m': temperature,\n",
    "    'relative_humidity_2m': relative_humidity,\n",
    "    'wind_speed_10m': wind_speed,\n",
    "    'wind_direction_10m': wind_dir,\n",
    "    'shortwave_radiation': shortwave,\n",
    "    'longwave_radiation': longwave,\n",
    "    'precipitation': precip_hourly,\n",
    "})\n",
    "\n",
    "df = df[(df['timestamp'] >= start_ts) & (df['timestamp'] <= end_ts)].sort_values('timestamp').reset_index(drop=True)\n",
    "df['snow_depth'] = np.nan  # will be filled by SNODAS later\n",
    "\n",
    "print(f\"âœ“ Retrieved {len(df)} hourly records\")\n",
    "print(f\"  Selected grid cell: {metadata['selected_lat']:.4f}Â°N, {metadata['selected_lon']:.4f}Â°E\")\n",
    "print(f\"  Pressure field: {metadata['pressure_var']}\")\n",
    "if metadata['missing_days']:\n",
    "    print(f\"  âš  Missing days: {', '.join(metadata['missing_days'][:5])} ...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21f615d",
   "metadata": {},
   "source": [
    "## SNODAS Snow Depth Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e752589b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNODAS function (simplified - only shows errors when debug=False)\n",
    "\n",
    "def get_snodas_snow_depth(lat, lon, date_str, cache_dir=\"snodas_cache\", debug=False):\n",
    "\n",
    "    \"\"\"Download and extract SNODAS snow depth from NSIDC.\"\"\"\n",
    "\n",
    "    SNODAS_NODATA = -9999\n",
    "\n",
    "    # Grid configurations (detected from file size)\n",
    "    GRID_CONFIGS = {\n",
    "        'old': {'XMIN': -124.73375000000000, 'YMAX': 52.87458333333333,\n",
    "                'XMAX': -66.94208333333333, 'YMIN': 24.94958333333333,\n",
    "                'NCOLS': 6935, 'NROWS': 3351, 'name': 'Pre-Oct-2013'},\n",
    "        'new': {'XMIN': -124.73333333333333, 'YMAX': 52.87500000000000,\n",
    "                'XMAX': -66.94166666666667, 'YMIN': 24.95000000000000,\n",
    "                'NCOLS': 3353, 'NROWS': 3353, 'name': 'Post-Oct-2013'}\n",
    "    }\n",
    "\n",
    "    # Check location bounds\n",
    "    if lat < 24.95 or lat > 52.88 or lon < -124.74 or lon > -66.94:\n",
    "        return None\n",
    "\n",
    "    # Construct URL\n",
    "    tar_filename = f\"SNODAS_{date_str}.tar\"\n",
    "    data_base = \"https://noaadata.apps.nsidc.org/NOAA/G02158/masked\"\n",
    "    year = date_str[:4]\n",
    "    month = date_str[4:6]\n",
    "    month_names = [\"01_Jan\", \"02_Feb\", \"03_Mar\", \"04_Apr\", \"05_May\", \"06_Jun\",\n",
    "                   \"07_Jul\", \"08_Aug\", \"09_Sep\", \"10_Oct\", \"11_Nov\", \"12_Dec\"]\n",
    "    month_dir = month_names[int(month) - 1]\n",
    "    data_url = f\"{data_base}/{year}/{month_dir}/{tar_filename}\"\n",
    "\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    cache_path = os.path.join(cache_dir, tar_filename)\n",
    "\n",
    "    try:\n",
    "        # Download or use cache\n",
    "        if os.path.exists(cache_path):\n",
    "            with open(cache_path, 'rb') as f:\n",
    "                tar_data = BytesIO(f.read())\n",
    "        else:\n",
    "            if debug:\n",
    "                print(f\"  Downloading {date_str}...\")\n",
    "            with urllib.request.urlopen(data_url, timeout=60) as response:\n",
    "                tar_data = BytesIO(response.read())\n",
    "                with open(cache_path, 'wb') as f:\n",
    "                    f.write(tar_data.getvalue())\n",
    "            tar_data.seek(0)\n",
    "\n",
    "        # Extract and decompress\n",
    "        with tarfile.open(fileobj=tar_data, mode='r') as tar:\n",
    "            for member in tar.getmembers():\n",
    "                if '1036' in member.name and member.name.endswith('.dat.gz'):\n",
    "                    snow_depth_gz_file = tar.extractfile(member)\n",
    "                    break\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        with gzip.open(snow_depth_gz_file, 'rb') as gz_file:\n",
    "            data = gz_file.read()\n",
    "\n",
    "        # Detect grid from file size\n",
    "        num_values = len(data) // 2\n",
    "        grid_config = None\n",
    "        for config in GRID_CONFIGS.values():\n",
    "            if num_values == config['NCOLS'] * config['NROWS']:\n",
    "                grid_config = config\n",
    "                break\n",
    "\n",
    "        if grid_config is None:\n",
    "            return None\n",
    "\n",
    "        # Parse binary data\n",
    "        SNODAS_NCOLS = grid_config['NCOLS']\n",
    "        SNODAS_NROWS = grid_config['NROWS']\n",
    "        values = struct.unpack(f\">{SNODAS_NCOLS * SNODAS_NROWS}h\", data)\n",
    "        snow_depth_array = np.array(values).reshape((SNODAS_NROWS, SNODAS_NCOLS))\n",
    "\n",
    "        # Calculate grid coordinates\n",
    "        SNODAS_XMIN = grid_config['XMIN']\n",
    "        SNODAS_YMAX = grid_config['YMAX']\n",
    "        SNODAS_CELLSIZE_X = (grid_config['XMAX'] - SNODAS_XMIN) / SNODAS_NCOLS\n",
    "        SNODAS_CELLSIZE_Y = (SNODAS_YMAX - grid_config['YMIN']) / SNODAS_NROWS\n",
    "\n",
    "        col = int((lon - SNODAS_XMIN) / SNODAS_CELLSIZE_X)\n",
    "        row = int((SNODAS_YMAX - lat) / SNODAS_CELLSIZE_Y)\n",
    "        col = max(0, min(SNODAS_NCOLS - 1, col))\n",
    "        row = max(0, min(SNODAS_NROWS - 1, row))\n",
    "\n",
    "        # Extract value\n",
    "        snow_depth_raw = snow_depth_array[row, col]\n",
    "        if snow_depth_raw == SNODAS_NODATA or snow_depth_raw < 0:\n",
    "            return None\n",
    "\n",
    "        snow_depth_m = snow_depth_raw / 1000.0\n",
    "        return snow_depth_m if snow_depth_m >= 0.01 else 0.0\n",
    "\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(f\"  Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"âœ“ SNODAS function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a634a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download SNODAS snow depth data and inject into the forcing dataframe\n",
    "print(\"Downloading SNODAS snow depth data...\")\n",
    "print(f\"Location: {latitude}Â°N, {longitude}Â°E | Date range: {start_date} to {end_date}\")\n",
    "\n",
    "snodas_hours_replaced = 0\n",
    "\n",
    "if latitude < 24.95 or latitude > 52.83 or longitude < -124.73 or longitude > -66.95:\n",
    "    print(\"âš  Location outside SNODAS coverage. HS will remain zero.\")\n",
    "    use_snodas = False\n",
    "else:\n",
    "    use_snodas = True\n",
    "    snodas_snow_depth = {}\n",
    "    failed_dates = []\n",
    "\n",
    "    all_dates = []\n",
    "    current_date = start_dt\n",
    "    while current_date <= end_dt:\n",
    "        all_dates.append(current_date)\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "    total_dates = len(all_dates)\n",
    "    print(f\"Processing {total_dates} days...\")\n",
    "\n",
    "    if len(all_dates) > 0:\n",
    "        test_date_str = all_dates[0].strftime(\"%Y%m%d\")\n",
    "        print(f\"\n",
    "ðŸ” Testing first date ({test_date_str})...\")\n",
    "        test_result = get_snodas_snow_depth(latitude, longitude, test_date_str, debug=True)\n",
    "        print(f\"Result: {test_result}\n",
    "\")\n",
    "\n",
    "    for idx, current_date in enumerate(all_dates, 1):\n",
    "        date_str = current_date.strftime(\"%Y%m%d\")\n",
    "        progress = (idx / total_dates) * 100\n",
    "\n",
    "        print(f\"[{idx:3d}/{total_dates}] ({progress:5.1f}%) {date_str}... \", end=\"\", flush=True)\n",
    "\n",
    "        try:\n",
    "            snow_depth = get_snodas_snow_depth(latitude, longitude, date_str, debug=False)\n",
    "            if snow_depth is not None:\n",
    "                snodas_snow_depth[date_str] = snow_depth\n",
    "                print(f\"âœ“ {snow_depth:.3f} m\")\n",
    "            else:\n",
    "                failed_dates.append(date_str)\n",
    "                print(\"âœ— No data\")\n",
    "        except Exception:\n",
    "            failed_dates.append(date_str)\n",
    "            print(\"âœ— Error\")\n",
    "\n",
    "    if len(snodas_snow_depth) == 0:\n",
    "        print(\"\n",
    "âš  No SNODAS data available. HS will stay at zero.\")\n",
    "        use_snodas = False\n",
    "    else:\n",
    "        print(f\"\n",
    "âœ“ Retrieved SNODAS snow depth for {len(snodas_snow_depth)}/{total_dates} days\")\n",
    "        if len(failed_dates) > 0:\n",
    "            print(f\"âš  {len(failed_dates)} dates failed\")\n",
    "\n",
    "        replaced_count = 0\n",
    "        for idx, row in df.iterrows():\n",
    "            date_str = row['timestamp'].strftime(\"%Y%m%d\")\n",
    "            if date_str in snodas_snow_depth:\n",
    "                df.at[idx, 'snow_depth'] = snodas_snow_depth[date_str] * 1000.0  # mm\n",
    "                replaced_count += 1\n",
    "        snodas_hours_replaced = replaced_count\n",
    "        print(f\"âœ“ Replaced HS values for {replaced_count} hourly records\")\n",
    "\n",
    "if not use_snodas:\n",
    "    df['snow_depth'] = df['snow_depth'].fillna(0.0)\n",
    "    print(\"\n",
    "Note: HS filled with zeros because SNODAS data was unavailable.\")\n",
    "else:\n",
    "    remaining = df['snow_depth'].isna().sum()\n",
    "    if remaining > 0:\n",
    "        df['snow_depth'] = df['snow_depth'].fillna(0.0)\n",
    "        print(f\"âš  {remaining} hours missing SNODAS values were filled with zeros.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeac2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to SMET format with ILWR included\n",
    "print(\"Converting data to SMET format...\")\n",
    "\n",
    "smet_df = pd.DataFrame()\n",
    "smet_df['timestamp'] = df['timestamp'].dt.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "smet_df['TA'] = df['temperature_2m']\n",
    "smet_df['RH'] = df['relative_humidity_2m']\n",
    "smet_df['VW'] = df['wind_speed_10m']\n",
    "smet_df['DW'] = df['wind_direction_10m']\n",
    "\n",
    "if use_snodas and snodas_hours_replaced > 0:\n",
    "    smet_df['HS'] = df['snow_depth'] / 1000.0\n",
    "    print(f\"  âœ“ Using SNODAS snow depth ({snodas_hours_replaced} hourly samples)\")\n",
    "else:\n",
    "    smet_df['HS'] = df['snow_depth'].fillna(0.0) / 1000.0\n",
    "    print(\"  âš  SNODAS unavailable - HS values are zeros\")\n",
    "\n",
    "smet_df['ISWR'] = df['shortwave_radiation']\n",
    "smet_df['ILWR'] = df['longwave_radiation']\n",
    "smet_df['PSUM'] = df['precipitation']\n",
    "\n",
    "nodata_value = -999\n",
    "smet_df = smet_df.fillna(nodata_value)\n",
    "\n",
    "numeric_cols = ['TA', 'RH', 'VW', 'DW', 'HS', 'ISWR', 'ILWR', 'PSUM']\n",
    "for col in numeric_cols:\n",
    "    smet_df[col] = pd.to_numeric(smet_df[col], errors='coerce').fillna(nodata_value)\n",
    "\n",
    "print(f\"âœ“ Data converted to SMET format\")\n",
    "print(f\"  Records: {len(smet_df)}\")\n",
    "\n",
    "fields = ['timestamp', 'TA', 'RH', 'VW', 'DW', 'HS', 'ISWR', 'ILWR', 'PSUM']\n",
    "smet_filename = f\"{station_id}_aorc_{start_date}_{end_date}.smet\"\n",
    "\n",
    "with open(smet_filename, 'w') as f:\n",
    "    f.write('SMET 1.2 ASCII\n",
    "')\n",
    "    f.write('[HEADER]\n",
    "')\n",
    "    f.write(f\"station_id = {station_id}\n",
    "\")\n",
    "    f.write(f\"latitude = {latitude:.10f}\n",
    "\")\n",
    "    f.write(f\"longitude = {longitude:.10f}\n",
    "\")\n",
    "    f.write(f\"altitude = {altitude:.2f}\n",
    "\")\n",
    "    f.write(f\"nodata = {nodata_value}\n",
    "\")\n",
    "    f.write('tz = 0\n",
    "')\n",
    "    f.write(f\"fields = {' '.join(fields)}\n",
    "\")\n",
    "    f.write('[DATA]\n",
    "')\n",
    "\n",
    "    for _, row in smet_df.iterrows():\n",
    "        values = []\n",
    "        for field in fields:\n",
    "            val = row[field]\n",
    "            if field == 'timestamp':\n",
    "                values.append(str(val))\n",
    "            else:\n",
    "                values.append(f\"{nodata_value}\" if pd.isna(val) or val == nodata_value else f\"{val:.2f}\")\n",
    "        f.write('\t'.join(values) + '\n",
    "')\n",
    "\n",
    "file_size = os.path.getsize(smet_filename)\n",
    "print(f\"\n",
    "âœ“ SMET file written: {smet_filename}\")\n",
    "print(f\"  File size: {file_size/1024:.2f} KB\")\n",
    "print(f\"\n",
    "First few records:\")\n",
    "print(smet_df.head())\n",
    "print(f\"\n",
    "Data summary:\")\n",
    "print(smet_df[numeric_cols].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f16e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary time series plots\n",
    "plot_df = smet_df.copy()\n",
    "plot_df['timestamp'] = pd.to_datetime(plot_df['timestamp'])\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "ax1 = axes[0]\n",
    "ax1.plot(plot_df['timestamp'], plot_df['TA'] - 273.15, color='red', linewidth=1.5, label='Temperature')\n",
    "ax1.set_ylabel('Temperature (Â°C)', fontsize=12)\n",
    "ax1.set_title(f'AORC Forcing Time Series - {station_id}', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "ax2 = axes[1]\n",
    "ax2.plot(plot_df['timestamp'], plot_df['HS'], color='blue', linewidth=1.5, label='SNODAS HS', alpha=0.8)\n",
    "ax2.set_ylabel('Snow Depth (m)', fontsize=12)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend(loc='upper left')\n",
    "\n",
    "ax3 = axes[2]\n",
    "ax3.bar(plot_df['timestamp'], plot_df['PSUM'], color='green', alpha=0.7, width=0.03, label='Hourly PSUM')\n",
    "ax3.set_ylabel('Precipitation (mm)', fontsize=12)\n",
    "ax3.set_xlabel('Time', fontsize=12)\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "ax3.legend(loc='upper left')\n",
    "\n",
    "plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('âœ“ Time series plots created')\n",
    "print(f\"  Snow depth range: {plot_df['HS'].min():.3f} - {plot_df['HS'].max():.3f} m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cc6992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional diagnostics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10), sharex=True)\n",
    "\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(plot_df['timestamp'], plot_df['VW'], color='purple', linewidth=1.5, label='Wind Speed')\n",
    "ax1.set_ylabel('Wind Speed (m/s)', fontsize=11)\n",
    "ax1.set_title('Wind Speed', fontsize=12)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(plot_df['timestamp'], plot_df['DW'], color='orange', linewidth=1.2, label='Wind Direction')\n",
    "ax2.set_ylabel('Wind Direction (Â°)', fontsize=11)\n",
    "ax2.set_title('Wind Direction', fontsize=12)\n",
    "ax2.set_ylim(0, 360)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(plot_df['timestamp'], plot_df['RH'] * 100, color='teal', linewidth=1.5, label='Relative Humidity')\n",
    "ax3.set_ylabel('Relative Humidity (%)', fontsize=11)\n",
    "ax3.set_title('Relative Humidity', fontsize=12)\n",
    "ax3.set_ylim(0, 100)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.legend()\n",
    "\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(plot_df['timestamp'], plot_df['ISWR'], color='gold', linewidth=1.2, label='ISWR')\n",
    "ax4.plot(plot_df['timestamp'], plot_df['ILWR'], color='brown', linewidth=1.2, label='ILWR')\n",
    "ax4.set_ylabel('Radiation (W/mÂ²)', fontsize=11)\n",
    "ax4.set_title('Shortwave vs Longwave', fontsize=12)\n",
    "ax4.set_xlabel('Time', fontsize=11)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.legend()\n",
    "\n",
    "plt.setp(ax4.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('âœ“ Additional plots created')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf2d66a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Fields in SMET: timestamp, TA, RH, VW, DW, HS (SNODAS), ISWR, ILWR, PSUM\n",
    "- HS (snow depth) sourced from SNODAS daily tarballs and converted to meters\n",
    "- PSUM represents hourly precipitation increments derived from the cumulative AORC field\n",
    "- ILWR (downward longwave) is now written alongside ISWR for full-radiation forcing"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}